{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing with Vertector\n",
    "\n",
    "This notebook demonstrates document processing capabilities including:\n",
    "- PDF, DOCX, PPTX, XLSX processing\n",
    "- Classic vs VLM pipeline selection\n",
    "- Export to multiple formats\n",
    "- Table extraction\n",
    "- OCR configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 15:57:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.monitoring.logger\u001b[0m:\u001b[36msetup_logging\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mLogging initialized at INFO level\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from vertector_data_ingestion import (\n",
    "    UniversalConverter,\n",
    "    LocalMpsConfig,\n",
    "    CloudGpuConfig,\n",
    "    CloudCpuConfig,\n",
    "    ExportFormat,\n",
    "    PipelineType,\n",
    "    HardwareDetector,\n",
    "    setup_logging,\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Detection\n",
    "\n",
    "First, let's detect available hardware to optimize our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 15:57:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2025-12-31 15:57:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware Information:\n",
      "==================================================\n",
      "device_type: mps\n",
      "batch_size: 8\n",
      "use_fp16: False\n",
      "num_workers: 4\n",
      "use_mlx: True\n",
      "platform: darwin\n",
      "chip: M1\n",
      "\n",
      "Recommended device: HardwareType.MPS\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware\n",
    "hw_info = HardwareDetector.get_device_info()\n",
    "\n",
    "print(\"Hardware Information:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in hw_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Get optimized config\n",
    "hw_config = HardwareDetector.detect()\n",
    "print(f\"\\nRecommended device: {hw_config.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Document Conversion\n",
    "\n",
    "Convert a document using default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 15:57:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2025-12-31 15:57:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2025-12-31 15:57:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mHardware detected: mps\u001b[0m\n",
      "\u001b[32m2025-12-31 15:57:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mInitialized UniversalConverter on mps\u001b[0m\n",
      "\u001b[32m2025-12-31 15:57:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_ensure_models_available\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mChecking model availability...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize converter with hardware-optimized config\n",
    "config = LocalMpsConfig()  # or CloudGpuConfig() or CloudCpuConfig()\n",
    "converter = UniversalConverter(config)\n",
    "\n",
    "# Convert a document (replace with your file)\n",
    "# Note: For audio files (.wav, .mp3), install the 'asr' extra: uv sync --extra asr\n",
    "doc_path = Path(\"../test_documents/arxiv_sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 15:59:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mUsing Classic pipeline for PDF with tables: arxiv_sample.pdf\u001b[0m\n",
      "\u001b[32m2025-12-31 15:59:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with classic pipeline\u001b[0m\n",
      "2025-12-31 15:59:02,610 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-31 15:59:02,697 - INFO - Going to convert document batch...\n",
      "2025-12-31 15:59:02,698 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2025-12-31 15:59:02,737 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-31 15:59:02,740 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-31 15:59:02,748 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-31 15:59:02,761 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-31 15:59:12,081 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-31 15:59:12,086 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-31 15:59:12,099 - INFO - Accelerator device: 'mps'\n",
      "2025-12-31 15:59:24,467 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-31 15:59:24,470 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-31 16:00:01,306 - INFO - Accelerator device: 'mps'\n",
      "2025-12-31 16:00:01,904 - INFO - Processing document arxiv_sample.pdf\n",
      "2025-12-31 16:00:14,603 - INFO - Finished converting document arxiv_sample.pdf in 71.99 sec.\n",
      "\u001b[32m2025-12-31 16:00:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 121.70s (9 pages, 0.1 pages/sec)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: arxiv_sample.pdf\n",
      "Pages: 9\n",
      "Pipeline: classic\n",
      "Processing time: 121.70s\n"
     ]
    }
   ],
   "source": [
    "if doc_path.exists():\n",
    "    # Use unified convert() method\n",
    "    doc = converter.convert(doc_path)\n",
    "    \n",
    "    print(f\"Document: {doc.metadata.source_path.name}\")\n",
    "    print(f\"Pages: {doc.metadata.num_pages}\")\n",
    "    print(f\"Pipeline: {doc.metadata.pipeline_type}\")\n",
    "    print(f\"Processing time: {doc.metadata.processing_time:.2f}s\")\n",
    "else:\n",
    "    print(f\"File not found: {doc_path}\")\n",
    "    print(\"\\nReplace with path to your PDF, DOCX, PPTX, or other document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Different Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Output (first 500 chars):\n",
      "==================================================\n",
      "## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
      "\n",
      "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
      "\n",
      "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
      "\n",
      "Ahmed S. Nassar IBM Research\n",
      "\n",
      "Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
      "\n",
      "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
      "\n",
      "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
      "\n",
      "Figure 1: Four examples of complex page layouts across diff\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 16:00:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mUsing Classic pipeline for PDF with tables: arxiv_sample.pdf\u001b[0m\n",
      "\u001b[32m2025-12-31 16:00:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with classic pipeline\u001b[0m\n",
      "2025-12-31 16:00:41,831 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-31 16:00:41,836 - INFO - Going to convert document batch...\n",
      "2025-12-31 16:00:41,836 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2025-12-31 16:00:41,836 - INFO - Accelerator device: 'mps'\n",
      "2025-12-31 16:00:42,408 - INFO - Accelerator device: 'mps'\n",
      "2025-12-31 16:00:43,006 - INFO - Processing document arxiv_sample.pdf\n",
      "2025-12-31 16:00:54,825 - INFO - Finished converting document arxiv_sample.pdf in 13.00 sec.\n",
      "\u001b[32m2025-12-31 16:00:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 14.62s (9 pages, 0.6 pages/sec)\u001b[0m\n",
      "\u001b[32m2025-12-31 16:00:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36mconvert_and_export\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mExported to output/document.md\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: output/document.md\n"
     ]
    }
   ],
   "source": [
    "if doc_path.exists():\n",
    "    # Export to Markdown\n",
    "    markdown = converter.export(doc, ExportFormat.MARKDOWN)\n",
    "    print(\"Markdown Output (first 500 chars):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(markdown[:500])\n",
    "    print(\"...\\n\")\n",
    "    \n",
    "    # Save to file - output_dir is automatically managed\n",
    "    output_path = converter.convert_and_export(\n",
    "        source=doc_path,\n",
    "        output_name=\"document.md\",\n",
    "        format=ExportFormat.MARKDOWN\n",
    "    )\n",
    "    print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Selection\n",
    "\n",
    "Compare Classic vs VLM pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 16:05:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with classic pipeline\u001b[0m\n",
      "2025-12-31 16:05:14,310 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-31 16:05:14,316 - INFO - Going to convert document batch...\n",
      "2025-12-31 16:05:14,317 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2025-12-31 16:05:14,318 - INFO - Accelerator device: 'mps'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic Pipeline:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 16:05:15,260 - INFO - Accelerator device: 'mps'\n",
      "2025-12-31 16:05:15,740 - INFO - Processing document arxiv_sample.pdf\n",
      "2025-12-31 16:05:28,163 - INFO - Finished converting document arxiv_sample.pdf in 13.85 sec.\n",
      "\u001b[32m2025-12-31 16:05:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 13.86s (9 pages, 0.6 pages/sec)\u001b[0m\n",
      "\u001b[32m2025-12-31 16:05:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m_build_vlm_options\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1mUsing Granite-Docling-MLX (258M params, default)\u001b[0m\n",
      "\u001b[32m2025-12-31 16:05:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with vlm pipeline\u001b[0m\n",
      "2025-12-31 16:05:28,173 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-31 16:05:28,177 - INFO - Going to convert document batch...\n",
      "2025-12-31 16:05:28,178 - INFO - Initializing pipeline for VlmPipeline with options hash 40c755e91ed89bcff77c660f7fe19c81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time: 13.87s\n",
      "\n",
      "VLM Pipeline:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 16:05:28,858 - INFO - Processing document arxiv_sample.pdf\n",
      "2025-12-31 16:08:18,440 - INFO - Finished converting document arxiv_sample.pdf in 170.27 sec.\n",
      "\u001b[32m2025-12-31 16:08:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 170.27s (9 pages, 0.1 pages/sec)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time: 170.27s\n",
      "\n",
      "VLM is 12.28x slower (but more accurate)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if doc_path.exists():\n",
    "    print(\"Classic Pipeline:\")\n",
    "    start = time.time()\n",
    "    classic_doc = converter.convert(doc_path, use_vlm=False)\n",
    "    classic_time = time.time() - start\n",
    "    print(f\"  Time: {classic_time:.2f}s\")\n",
    "    \n",
    "    print(\"\\nVLM Pipeline:\")\n",
    "    start = time.time()\n",
    "    vlm_doc = converter.convert(doc_path, use_vlm=True)\n",
    "    vlm_time = time.time() - start\n",
    "    print(f\"  Time: {vlm_time:.2f}s\")\n",
    "    print(f\"\\nVLM is {vlm_time/classic_time:.2f}x slower (but more accurate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Process multiple documents with the same unified API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 16:09:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36mconvert_batch\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mConverting 3 documents in parallel\u001b[0m\n",
      "\u001b[32m2025-12-31 16:09:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mUsing VLM pipeline for scanned PDF: ocr_test.pdf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 16:09:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUsing Classic pipeline (default) for 2112.13734v2.pdf\u001b[0m\n",
      "\u001b[32m2025-12-31 16:09:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m_build_vlm_options\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1mUsing Granite-Docling-MLX (258M params, default)\u001b[0m\n",
      "\u001b[32m2025-12-31 16:09:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mConverting 2112.13734v2.pdf with classic pipeline\u001b[0m\n",
      "\u001b[32m2025-12-31 16:09:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mConverting ocr_test.pdf with vlm pipeline\u001b[0m\n",
      "2025-12-31 16:09:37,055 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-31 16:09:37,098 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-31 16:09:37,149 - INFO - Going to convert document batch...\n",
      "2025-12-31 16:09:37,191 - INFO - Going to convert document batch...\n",
      "2025-12-31 16:09:37,252 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2025-12-31 16:09:37,372 - INFO - Accelerator device: 'mps'\n",
      "\u001b[32m2025-12-31 16:09:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mUsing Classic pipeline for PDF with tables: arxiv_sample.pdf\u001b[0m\n",
      "\u001b[32m2025-12-31 16:09:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with classic pipeline\u001b[0m\n",
      "2025-12-31 16:09:37,895 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-31 16:09:37,905 - INFO - Going to convert document batch...\n",
      "2025-12-31 16:09:39,228 - INFO - Accelerator device: 'mps'\n",
      "2025-12-31 16:09:39,819 - INFO - Processing document 2112.13734v2.pdf\n",
      "2025-12-31 16:09:39,819 - INFO - Initializing pipeline for VlmPipeline with options hash 40c755e91ed89bcff77c660f7fe19c81\n",
      "2025-12-31 16:09:41,036 - INFO - Processing document ocr_test.pdf\n",
      "2025-12-31 16:09:41,036 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2025-12-31 16:09:41,038 - INFO - Accelerator device: 'mps'\n",
      "2025-12-31 16:09:44,520 - INFO - Accelerator device: 'mps'\n",
      "2025-12-31 16:09:45,298 - INFO - Processing document arxiv_sample.pdf\n",
      "2025-12-31 16:09:45,764 - INFO - Finished converting document ocr_test.pdf in 8.67 sec.\n",
      "\u001b[32m2025-12-31 16:09:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mConverted ocr_test.pdf in 11.98s (1 pages, 0.1 pages/sec)\u001b[0m\n",
      "2025-12-31 16:09:52,259 - INFO - Finished converting document 2112.13734v2.pdf in 15.21 sec.\n",
      "\u001b[32m2025-12-31 16:09:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mConverted 2112.13734v2.pdf in 18.50s (4 pages, 0.2 pages/sec)\u001b[0m\n",
      "2025-12-31 16:09:59,102 - INFO - Finished converting document arxiv_sample.pdf in 21.21 sec.\n",
      "\u001b[32m2025-12-31 16:09:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 25.32s (9 pages, 0.4 pages/sec)\u001b[0m\n",
      "\u001b[32m2025-12-31 16:09:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36mconvert_batch\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mBatch conversion complete: 3 successful, 0 failed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  ocr_test.pdf: 1 pages\n",
      "  2112.13734v2.pdf: 4 pages\n",
      "  arxiv_sample.pdf: 9 pages\n"
     ]
    }
   ],
   "source": [
    "documents_dir = Path(\"../test_documents/\")\n",
    "\n",
    "if documents_dir.exists():\n",
    "    pdf_files = list(documents_dir.glob(\"*.pdf\"))[:5]\n",
    "    \n",
    "    if pdf_files:\n",
    "        print(f\"Processing {len(pdf_files)} documents...\")\n",
    "        \n",
    "        # Same convert() method - just pass a list!\n",
    "        results = converter.convert(pdf_files, parallel=True)\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        for doc in results:\n",
    "            print(f\"  {doc.metadata.source_path.name}: {doc.metadata.num_pages} pages\")\n",
    "    else:\n",
    "        print(\"No PDF files found\")\n",
    "else:\n",
    "    print(\"Create a 'documents/' directory with PDFs to test batch processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Unified `convert()` API for single and batch processing\n",
    "- Hardware detection\n",
    "- Pipeline selection\n",
    "- Export capabilities\n",
    "\n",
    "Next: Check out `02_audio_transcription.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
