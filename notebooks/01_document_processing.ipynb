{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing with Vertector\n",
    "\n",
    "This notebook demonstrates document processing capabilities including:\n",
    "- PDF, DOCX, PPTX, XLSX processing\n",
    "- Classic vs VLM pipeline selection\n",
    "- Export to multiple formats\n",
    "- Table extraction\n",
    "- OCR configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 18:37:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.monitoring.logger\u001b[0m:\u001b[36msetup_logging\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mLogging initialized at INFO level\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from vertector_data_ingestion import (\n",
    "    UniversalConverter,\n",
    "    LocalMpsConfig,\n",
    "    CloudGpuConfig,\n",
    "    CloudCpuConfig,\n",
    "    ExportFormat,\n",
    "    PipelineType,\n",
    "    HardwareDetector,\n",
    "    setup_logging,\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Detection\n",
    "\n",
    "First, let's detect available hardware to optimize our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 18:38:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-02 18:38:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware Information:\n",
      "==================================================\n",
      "device_type: mps\n",
      "batch_size: 8\n",
      "use_fp16: False\n",
      "num_workers: 4\n",
      "use_mlx: True\n",
      "platform: darwin\n",
      "chip: M1\n",
      "\n",
      "Recommended device: HardwareType.MPS\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware\n",
    "hw_info = HardwareDetector.get_device_info()\n",
    "\n",
    "print(\"Hardware Information:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in hw_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Get optimized config\n",
    "hw_config = HardwareDetector.detect()\n",
    "print(f\"\\nRecommended device: {hw_config.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Document Conversion\n",
    "\n",
    "Convert a document using default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 18:38:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-02 18:38:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-02 18:38:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mHardware detected: mps\u001b[0m\n",
      "\u001b[32m2026-01-02 18:38:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mInitialized UniversalConverter on mps\u001b[0m\n",
      "\u001b[32m2026-01-02 18:38:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_ensure_models_available\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mChecking model availability...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize converter with hardware-optimized config\n",
    "config = LocalMpsConfig()  # or CloudGpuConfig() or CloudCpuConfig()\n",
    "converter = UniversalConverter(config)\n",
    "\n",
    "# Convert a document (replace with your file)\n",
    "# Note: For audio files (.wav, .mp3), install the 'asr' extra: uv sync --extra asr\n",
    "doc_path = Path(\"../test_documents/arxiv_sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 18:38:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mUsing Classic pipeline for PDF with tables: arxiv_sample.pdf\u001b[0m\n",
      "\u001b[32m2026-01-02 18:38:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with classic pipeline\u001b[0m\n",
      "2026-01-02 18:38:21,570 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 18:38:21,665 - INFO - Going to convert document batch...\n",
      "2026-01-02 18:38:21,666 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-02 18:38:21,693 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 18:38:21,697 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-02 18:38:21,707 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 18:38:21,725 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-02 18:38:27,993 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 18:38:27,999 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-02 18:38:28,016 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 18:38:29,243 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 18:38:29,249 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-02 18:38:46,072 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 18:38:47,132 - INFO - Processing document arxiv_sample.pdf\n",
      "2026-01-02 18:39:07,791 - INFO - Finished converting document arxiv_sample.pdf in 46.22 sec.\n",
      "\u001b[32m2026-01-02 18:39:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 51.53s (9 pages, 0.2 pages/sec)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: arxiv_sample.pdf\n",
      "Pages: 9\n",
      "Pipeline: classic\n",
      "Processing time: 51.53s\n"
     ]
    }
   ],
   "source": [
    "if doc_path.exists():\n",
    "    # Use unified convert() method\n",
    "    doc = converter.convert(doc_path)\n",
    "    \n",
    "    print(f\"Document: {doc.metadata.source_path.name}\")\n",
    "    print(f\"Pages: {doc.metadata.num_pages}\")\n",
    "    print(f\"Pipeline: {doc.metadata.pipeline_type}\")\n",
    "    print(f\"Processing time: {doc.metadata.processing_time:.2f}s\")\n",
    "else:\n",
    "    print(f\"File not found: {doc_path}\")\n",
    "    print(\"\\nReplace with path to your PDF, DOCX, PPTX, or other document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Different Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Output (first 500 chars):\n",
      "==================================================\n",
      "## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
      "\n",
      "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
      "\n",
      "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
      "\n",
      "Ahmed S. Nassar IBM Research\n",
      "\n",
      "Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
      "\n",
      "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
      "\n",
      "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
      "\n",
      "Figure 1: Four examples of complex page layouts across diff\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 18:46:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mUsing Classic pipeline for PDF with tables: arxiv_sample.pdf\u001b[0m\n",
      "\u001b[32m2026-01-02 18:46:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with classic pipeline\u001b[0m\n",
      "2026-01-02 18:46:28,624 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 18:46:28,631 - INFO - Going to convert document batch...\n",
      "2026-01-02 18:46:28,632 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-02 18:46:28,632 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 18:46:30,217 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 18:46:31,121 - INFO - Processing document arxiv_sample.pdf\n",
      "2026-01-02 18:46:51,391 - INFO - Finished converting document arxiv_sample.pdf in 22.77 sec.\n",
      "\u001b[32m2026-01-02 18:46:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 25.29s (9 pages, 0.4 pages/sec)\u001b[0m\n",
      "\u001b[32m2026-01-02 18:46:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36mconvert_and_export\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mExported to output/document.md\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: output/document.md\n"
     ]
    }
   ],
   "source": [
    "if doc_path.exists():\n",
    "    # Export to Markdown\n",
    "    markdown = converter.export(doc, ExportFormat.MARKDOWN)\n",
    "    print(\"Markdown Output (first 500 chars):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(markdown[:500])\n",
    "    print(\"...\\n\")\n",
    "    \n",
    "    # Save to file - output_dir is automatically managed\n",
    "    output_path = converter.convert_and_export(\n",
    "        source=doc_path,\n",
    "        output_name=\"document.md\",\n",
    "        format=ExportFormat.MARKDOWN\n",
    "    )\n",
    "    print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Selection\n",
    "\n",
    "Compare Classic vs VLM pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 18:48:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with classic pipeline\u001b[0m\n",
      "2026-01-02 18:48:05,091 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 18:48:05,100 - INFO - Going to convert document batch...\n",
      "2026-01-02 18:48:05,101 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-02 18:48:05,102 - INFO - Accelerator device: 'mps'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic Pipeline:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 18:48:05,820 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 18:48:06,643 - INFO - Processing document arxiv_sample.pdf\n",
      "2026-01-02 18:48:25,493 - INFO - Finished converting document arxiv_sample.pdf in 20.40 sec.\n",
      "\u001b[32m2026-01-02 18:48:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 20.41s (9 pages, 0.4 pages/sec)\u001b[0m\n",
      "\u001b[32m2026-01-02 18:48:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m_build_vlm_options\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mUsing Granite-Docling-MLX (258M params, default)\u001b[0m\n",
      "\u001b[32m2026-01-02 18:48:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with vlm pipeline\u001b[0m\n",
      "2026-01-02 18:48:25,504 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 18:48:25,510 - INFO - Going to convert document batch...\n",
      "2026-01-02 18:48:25,511 - INFO - Initializing pipeline for VlmPipeline with options hash 40c755e91ed89bcff77c660f7fe19c81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time: 20.42s\n",
      "\n",
      "VLM Pipeline:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 18:48:26,727 - INFO - Processing document arxiv_sample.pdf\n",
      "2026-01-02 18:53:43,198 - INFO - Finished converting document arxiv_sample.pdf in 317.70 sec.\n",
      "\u001b[32m2026-01-02 18:53:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 317.70s (9 pages, 0.0 pages/sec)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time: 317.72s\n",
      "\n",
      "VLM is 15.56x slower (but more accurate)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if doc_path.exists():\n",
    "    print(\"Classic Pipeline:\")\n",
    "    start = time.time()\n",
    "    classic_doc = converter.convert(doc_path, use_vlm=False)\n",
    "    classic_time = time.time() - start\n",
    "    print(f\"  Time: {classic_time:.2f}s\")\n",
    "    \n",
    "    print(\"\\nVLM Pipeline:\")\n",
    "    start = time.time()\n",
    "    vlm_doc = converter.convert(doc_path, use_vlm=True)\n",
    "    vlm_time = time.time() - start\n",
    "    print(f\"  Time: {vlm_time:.2f}s\")\n",
    "    print(f\"\\nVLM is {vlm_time/classic_time:.2f}x slower (but more accurate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Process multiple documents with the same unified API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 18:54:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36mconvert_batch\u001b[0m:\u001b[36m321\u001b[0m - \u001b[1mConverting 3 documents in parallel\u001b[0m\n",
      "\u001b[32m2026-01-02 18:54:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mUsing VLM pipeline for scanned PDF: ocr_test.pdf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 18:54:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mUsing Classic pipeline (default) for 2112.13734v2.pdf\u001b[0m\n",
      "\u001b[32m2026-01-02 18:54:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m_build_vlm_options\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mUsing Granite-Docling-MLX (258M params, default)\u001b[0m\n",
      "\u001b[32m2026-01-02 18:54:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting 2112.13734v2.pdf with classic pipeline\u001b[0m\n",
      "\u001b[32m2026-01-02 18:54:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting ocr_test.pdf with vlm pipeline\u001b[0m\n",
      "2026-01-02 18:54:17,681 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 18:54:17,712 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 18:54:17,758 - INFO - Going to convert document batch...\n",
      "2026-01-02 18:54:17,809 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-02 18:54:17,822 - INFO - Going to convert document batch...\n",
      "2026-01-02 18:54:17,877 - INFO - Accelerator device: 'mps'\n",
      "\u001b[32m2026-01-02 18:54:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mUsing Classic pipeline for PDF with tables: arxiv_sample.pdf\u001b[0m\n",
      "\u001b[32m2026-01-02 18:54:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with classic pipeline\u001b[0m\n",
      "2026-01-02 18:54:19,610 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 18:54:19,627 - INFO - Going to convert document batch...\n",
      "2026-01-02 18:54:21,090 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 18:54:22,046 - INFO - Processing document 2112.13734v2.pdf\n",
      "2026-01-02 18:54:22,046 - INFO - Initializing pipeline for VlmPipeline with options hash 40c755e91ed89bcff77c660f7fe19c81\n",
      "2026-01-02 18:54:23,599 - INFO - Processing document ocr_test.pdf\n",
      "2026-01-02 18:54:23,599 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-02 18:54:23,602 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 18:54:32,003 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 18:54:34,043 - INFO - Processing document arxiv_sample.pdf\n",
      "2026-01-02 18:54:34,974 - INFO - Finished converting document ocr_test.pdf in 17.27 sec.\n",
      "\u001b[32m2026-01-02 18:54:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted ocr_test.pdf in 18.05s (1 pages, 0.1 pages/sec)\u001b[0m\n",
      "2026-01-02 18:54:48,462 - INFO - Finished converting document 2112.13734v2.pdf in 30.82 sec.\n",
      "\u001b[32m2026-01-02 18:54:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted 2112.13734v2.pdf in 31.59s (4 pages, 0.1 pages/sec)\u001b[0m\n",
      "2026-01-02 18:54:57,922 - INFO - Finished converting document arxiv_sample.pdf in 38.31 sec.\n",
      "\u001b[32m2026-01-02 18:54:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 41.03s (9 pages, 0.2 pages/sec)\u001b[0m\n",
      "\u001b[32m2026-01-02 18:54:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36mconvert_batch\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mBatch conversion complete: 3 successful, 0 failed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  ocr_test.pdf: 1 pages\n",
      "  2112.13734v2.pdf: 4 pages\n",
      "  arxiv_sample.pdf: 9 pages\n"
     ]
    }
   ],
   "source": [
    "documents_dir = Path(\"../test_documents/\")\n",
    "\n",
    "if documents_dir.exists():\n",
    "    pdf_files = list(documents_dir.glob(\"*.pdf\"))[:5]\n",
    "    \n",
    "    if pdf_files:\n",
    "        print(f\"Processing {len(pdf_files)} documents...\")\n",
    "        \n",
    "        # Same convert() method - just pass a list!\n",
    "        results = converter.convert(pdf_files, parallel=True)\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        for doc in results:\n",
    "            print(f\"  {doc.metadata.source_path.name}: {doc.metadata.num_pages} pages\")\n",
    "    else:\n",
    "        print(\"No PDF files found\")\n",
    "else:\n",
    "    print(\"Create a 'documents/' directory with PDFs to test batch processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Unified `convert()` API for single and batch processing\n",
    "- Hardware detection\n",
    "- Pipeline selection\n",
    "- Export capabilities\n",
    "\n",
    "Next: Check out `02_audio_transcription.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
