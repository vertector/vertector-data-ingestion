{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Transcription with Vertector\n",
    "\n",
    "Demonstrates:\n",
    "- Whisper model configuration\n",
    "- MLX vs Standard backend\n",
    "- Timestamped transcriptions\n",
    "- Multi-language support\n",
    "- Batch processing\n",
    "- SRT generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from vertector_data_ingestion import (\n",
    "    create_audio_transcriber,\n",
    "    AudioConfig,\n",
    "    WhisperModelSize,\n",
    "    AudioBackend,\n",
    "    HardwareDetector,\n",
    "    setup_logging,\n",
    ")\n",
    "\n",
    "setup_logging(log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_info = HardwareDetector.get_device_info()\n",
    "\n",
    "print(\"Hardware:\")\n",
    "print(f\"  Device: {hw_info.get('device_type')}\")\n",
    "print(f\"  Chip: {hw_info.get('chip', 'Unknown')}\")\n",
    "print(f\"  Use MLX: {hw_info.get('use_mlx', False)}\")\n",
    "print(f\"  Batch Size: {hw_info.get('batch_size', 1)}\")\n",
    "\n",
    "if hw_info.get('device_type') == 'mps':\n",
    "    print(\"\\n✓ Recommend: MLX backend (10-20x faster on Apple Silicon)\")\n",
    "elif hw_info.get('device_type') == 'cuda':\n",
    "    print(\"\\n✓ Recommend: Standard with CUDA\")\n",
    "else:\n",
    "    print(\"\\n✓ Recommend: Standard (CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AudioConfig(\n",
    "    model_size=WhisperModelSize.BASE,\n",
    "    backend=AudioBackend.AUTO,\n",
    "    language=\"en\",\n",
    "    word_timestamps=True,\n",
    ")\n",
    "\n",
    "transcriber = create_audio_transcriber(config)\n",
    "audio_path = Path(\"../test_documents/harvard.wav\")\n",
    "\n",
    "if audio_path.exists():\n",
    "    result = transcriber.transcribe(audio_path)\n",
    "    \n",
    "    print(\"Result:\")\n",
    "    print(f\"  Text: {result.text}\")\n",
    "    print(f\"  Language: {result.language}\")\n",
    "    print(f\"  Duration: {result.duration:.2f}s\")\n",
    "    print(f\"  Segments: {len(result.segments)}\")\n",
    "else:\n",
    "    print(f\"File not found: {audio_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamped Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if audio_path.exists():\n",
    "    for i, segment in enumerate(result.segments[:5], 1):\n",
    "        print(f\"\\nSegment {i}:\")\n",
    "        print(f\"  Time: [{segment.start:.1f}s - {segment.end:.1f}s]\")\n",
    "        print(f\"  Text: {segment.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if audio_path.exists():\n",
    "    models = [WhisperModelSize.TINY, WhisperModelSize.BASE, WhisperModelSize.SMALL]\n",
    "    \n",
    "    print(\"Model Comparison:\")\n",
    "    for model_size in models:\n",
    "        config = AudioConfig(model_size=model_size, backend=AudioBackend.AUTO)\n",
    "        transcriber = create_audio_transcriber(config)\n",
    "        \n",
    "        start = time.time()\n",
    "        result = transcriber.transcribe(audio_path)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"\\n{model_size.value.upper()}: {elapsed:.2f}s\")\n",
    "        print(f\"  Text: {result.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if audio_path.exists():\n",
    "    # Auto-detect\n",
    "    auto_config = AudioConfig(model_size=WhisperModelSize.BASE, language=None)\n",
    "    transcriber = create_audio_transcriber(auto_config)\n",
    "    result = transcriber.transcribe(audio_path)\n",
    "    \n",
    "    print(f\"Detected language: {result.language}\")\n",
    "    print(f\"Text: {result.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = Path(\"../test_documents/\")\n",
    "\n",
    "if audio_dir.exists():\n",
    "    audio_files = list(audio_dir.glob(\"*.wav\")) + list(audio_dir.glob(\"*.mp3\"))\n",
    "    \n",
    "    if audio_files:\n",
    "        config = AudioConfig(model_size=WhisperModelSize.BASE)\n",
    "        transcriber = create_audio_transcriber(config)\n",
    "        \n",
    "        for audio_file in audio_files[:5]:\n",
    "            result = transcriber.transcribe(audio_file)\n",
    "            print(f\"\\n{audio_file.name}: {result.duration:.1f}s\")\n",
    "            print(f\"  {result.text[:100]}...\")\n",
    "else:\n",
    "    print(\"Create '../test_documents/' directory with audio files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate SRT Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_srt_timestamp(seconds: float) -> str:\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    millis = int((seconds % 1) * 1000)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}\"\n",
    "\n",
    "if audio_path.exists():\n",
    "    config = AudioConfig(model_size=WhisperModelSize.BASE, word_timestamps=True)\n",
    "    transcriber = create_audio_transcriber(config)\n",
    "    result = transcriber.transcribe(audio_path)\n",
    "    \n",
    "    # Generate SRT\n",
    "    srt_output = []\n",
    "    for i, segment in enumerate(result.segments, 1):\n",
    "        start = format_srt_timestamp(segment.start)\n",
    "        end = format_srt_timestamp(segment.end)\n",
    "        srt_output.append(f\"{i}\\n{start} --> {end}\\n{segment.text.strip()}\\n\")\n",
    "    \n",
    "    srt_content = \"\\n\".join(srt_output)\n",
    "    \n",
    "    # Save using convert_and_export pattern\n",
    "    from vertector_data_ingestion import UniversalConverter\n",
    "    converter = UniversalConverter()\n",
    "    srt_path = converter.config.output_dir / \"transcript.srt\"\n",
    "    srt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    srt_path.write_text(srt_content, encoding=\"utf-8\")\n",
    "    \n",
    "    print(f\"Saved to: {srt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Demonstrated:\n",
    "- Hardware detection\n",
    "- Basic transcription\n",
    "- Timestamped segments\n",
    "- Model comparison\n",
    "- Multi-language support\n",
    "- Batch processing\n",
    "- SRT generation\n",
    "\n",
    "Next: `03_rag_pipeline.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
