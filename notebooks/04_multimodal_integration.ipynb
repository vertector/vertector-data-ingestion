{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multimodal Integration\n\nCombine documents and audio in a unified pipeline."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nfrom vertector_data_ingestion import (\n    UniversalConverter,\n    LocalMpsConfig,\n    HybridChunker,\n    ChromaAdapter,\n    create_audio_transcriber,\n    AudioConfig,\n    WhisperModelSize,\n    setup_logging,\n)\n\nsetup_logging(log_level=\"INFO\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Multimodal Pipeline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class MultimodalPipeline:\n    def __init__(self):\n        self.converter = UniversalConverter(LocalMpsConfig())\n        self.chunker = HybridChunker()\n        self.audio_transcriber = create_audio_transcriber(\n            AudioConfig(model_size=WhisperModelSize.BASE)\n        )\n        self.vector_store = ChromaAdapter(collection_name=\"multimodal\")\n    \n    def process_document(self, path: Path):\n        print(f\"Processing: {path.name}\")\n        doc = self.converter.convert(path)\n        chunks = self.chunker.chunk_document(doc.document)\n        \n        for chunk in chunks.chunks:\n            chunk.metadata[\"modality\"] = \"document\"\n            chunk.metadata[\"source\"] = path.name\n        \n        self.vector_store.add_chunks(chunks.chunks)\n        print(f\"  Added {len(chunks.chunks)} chunks\")\n        return len(chunks.chunks)\n    \n    def process_audio(self, path: Path):\n        print(f\"Processing: {path.name}\")\n        result = self.audio_transcriber.transcribe(path)\n        \n        from vertector_data_ingestion.models.chunk import DocumentChunk\n        chunks = []\n        for i, segment in enumerate(result.segments):\n            chunk = DocumentChunk(\n                chunk_id=f\"{path.stem}_{i}\",\n                text=segment.text,\n                metadata={\n                    \"modality\": \"audio\",\n                    \"source\": path.name,\n                    \"start_time\": segment.start,\n                    \"end_time\": segment.end,\n                }\n            )\n            chunks.append(chunk)\n        \n        self.vector_store.add_chunks(chunks)\n        print(f\"  Added {len(chunks)} chunks\")\n        return len(chunks)\n    \n    def search(self, query: str, top_k: int = 5):\n        return self.vector_store.search(query, top_k=top_k)\n\npipeline = MultimodalPipeline()\nprint(\"âœ“ Pipeline ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Process Documents and Audio"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process documents\ndoc_path = Path(\"document.pdf\")\nif doc_path.exists():\n    pipeline.process_document(doc_path)\n\n# Process audio\naudio_path = Path(\"audio.wav\")\nif audio_path.exists():\n    pipeline.process_audio(audio_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cross-Modal Search"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = pipeline.search(\"machine learning\", top_k=3)\n\nfor i, result in enumerate(results, 1):\n    modality = result['metadata'].get('modality', 'unknown')\n    source = result['metadata'].get('source', 'unknown')\n    \n    print(f\"\\nResult {i} [{modality.upper()}]:\")\n    print(f\"  Source: {source}\")\n    print(f\"  Text: {result['text'][:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nDemonstrated:\n- Unified multimodal pipeline\n- Document and audio processing\n- Cross-modal search\n\nSee documentation for more examples."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}