{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Integration\n",
    "\n",
    "Combine documents and audio in a unified pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 21:26:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.monitoring.logger\u001b[0m:\u001b[36msetup_logging\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mLogging initialized at INFO level\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from vertector_data_ingestion import (\n",
    "    UniversalConverter,\n",
    "    LocalMpsConfig,\n",
    "    HybridChunker,\n",
    "    ChromaAdapter,\n",
    "    create_audio_transcriber,\n",
    "    AudioConfig,\n",
    "    WhisperModelSize,\n",
    "    setup_logging,\n",
    ")\n",
    "\n",
    "from vertector_data_ingestion.models.config import ChunkingConfig\n",
    "\n",
    "setup_logging(log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 21:26:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mHardware detected: mps\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mInitialized UniversalConverter on mps\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_ensure_models_available\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mChecking model availability...\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading tokenizer: Qwen/Qwen3-Embedding-0.6B (padding_side=left)\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mInitialized HybridChunker with max_tokens=512, merge_peers=True\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.audio_factory\u001b[0m:\u001b[36mcreate_audio_transcriber\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mCreating audio transcriber: model=base, backend=auto\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mInitializing WhisperTranscriber with model=base, device=mlx, backend=auto\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mLoading embedding model: Qwen/Qwen3-Embedding-0.6B\u001b[0m\n",
      "2026-01-02 21:26:34,873 - INFO - Use pytorch device_name: mps\n",
      "2026-01-02 21:26:34,874 - INFO - Load pretrained SentenceTransformer: Qwen/Qwen3-Embedding-0.6B\n",
      "2026-01-02 21:26:40,853 - INFO - 1 prompt is loaded, with the key: query\n",
      "2026-01-02 21:26:40,884 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "\u001b[32m2026-01-02 21:26:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mChromaDB initialized in-memory\u001b[0m\n",
      "\u001b[32m2026-01-02 21:26:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mUsing collection: multimodal\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline ready\n"
     ]
    }
   ],
   "source": [
    "class MultimodalPipeline:\n",
    "    def __init__(self):\n",
    "        self.converter = UniversalConverter(LocalMpsConfig())\n",
    "        self.chunk_config = ChunkingConfig(\n",
    "            tokenizer=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "            max_tokens=512,\n",
    "        )\n",
    "        self.chunker = HybridChunker(config=self.chunk_config)\n",
    "        self.audio_transcriber = create_audio_transcriber(\n",
    "            AudioConfig(model_size=WhisperModelSize.BASE)\n",
    "        )\n",
    "        self.vector_store = ChromaAdapter(\n",
    "            collection_name=\"multimodal\",\n",
    "            embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n",
    "        )\n",
    "    \n",
    "    def process_document(self, path: Path):\n",
    "        print(f\"Processing: {path.name}\")\n",
    "        doc = self.converter.convert(path)\n",
    "        chunks = self.chunker.chunk_document(doc)\n",
    "        \n",
    "        for chunk in chunks.chunks:\n",
    "            chunk.metadata[\"modality\"] = \"document\"\n",
    "            chunk.metadata[\"source\"] = path.name\n",
    "        \n",
    "        self.vector_store.add_chunks(chunks.chunks)\n",
    "        print(f\"  Added {len(chunks.chunks)} chunks\")\n",
    "        return len(chunks.chunks)\n",
    "    \n",
    "    def process_audio(self, path: Path):\n",
    "        print(f\"Processing: {path.name}\")\n",
    "        result = self.audio_transcriber.transcribe(path)\n",
    "        \n",
    "        from vertector_data_ingestion.models.chunk import DocumentChunk\n",
    "        from transformers import AutoTokenizer\n",
    "        \n",
    "        # Load tokenizer for token counting\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.chunk_config.tokenizer)\n",
    "        \n",
    "        chunks = []\n",
    "        for i, segment in enumerate(result.segments):\n",
    "            # Count tokens in the segment text\n",
    "            tokens = tokenizer.encode(segment.text, add_special_tokens=False)\n",
    "            \n",
    "            chunk = DocumentChunk(\n",
    "                chunk_id=f\"{path.stem}_{i}\",\n",
    "                text=segment.text,\n",
    "                token_count=len(tokens),\n",
    "                source_path=path,\n",
    "                chunk_index=i,\n",
    "                metadata={\n",
    "                    \"modality\": \"audio\",\n",
    "                    \"source\": path.name,\n",
    "                    \"start_time\": segment.start,\n",
    "                    \"end_time\": segment.end,\n",
    "                    \"duration\": segment.end - segment.start,\n",
    "                }\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        self.vector_store.add_chunks(chunks)\n",
    "        print(f\"  Added {len(chunks)} audio segments\")\n",
    "        return len(chunks)\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5):\n",
    "        return self.vector_store.search(query, top_k=top_k)\n",
    "\n",
    "pipeline = MultimodalPipeline()\n",
    "print(\"✓ Pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Documents and Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 2112.13734v2.pdf\n",
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-02 21:27:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mUsing Classic pipeline (default) for 2112.13734v2.pdf\u001b[0m\n",
      "\u001b[32m2026-01-02 21:27:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting 2112.13734v2.pdf with classic pipeline\u001b[0m\n",
      "2026-01-02 21:27:35,939 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 21:27:36,012 - INFO - Going to convert document batch...\n",
      "2026-01-02 21:27:36,013 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-02 21:27:36,027 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 21:27:36,029 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-02 21:27:36,042 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 21:27:36,048 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-02 21:27:36,830 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 21:27:36,834 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-02 21:27:36,840 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 21:27:37,506 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 21:27:37,508 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-02 21:27:37,825 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 21:27:38,887 - INFO - Processing document 2112.13734v2.pdf\n",
      "2026-01-02 21:27:49,894 - INFO - Finished converting document 2112.13734v2.pdf in 13.96 sec.\n",
      "\u001b[32m2026-01-02 21:27:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted 2112.13734v2.pdf in 14.23s (4 pages, 0.3 pages/sec)\u001b[0m\n",
      "\u001b[32m2026-01-02 21:27:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mChunking document: 2112.13734v2.pdf (4 pages)\u001b[0m\n",
      "\u001b[32m2026-01-02 21:27:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mCreated 22 chunks\u001b[0m\n",
      "\u001b[32m2026-01-02 21:27:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36madd_chunks\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mAdding 22 chunks to ChromaDB (batch_size=16)\u001b[0m\n",
      "\u001b[32m2026-01-02 21:28:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36madd_chunks\u001b[0m:\u001b[36m147\u001b[0m - \u001b[1mSuccessfully added 22 chunks in 2 batches\u001b[0m\n",
      "\u001b[32m2026-01-02 21:28:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m_load_model\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mLoading Whisper model: base\u001b[0m\n",
      "\u001b[32m2026-01-02 21:28:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m_load_model\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mLoaded MLX Whisper model: base\u001b[0m\n",
      "\u001b[32m2026-01-02 21:28:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36mtranscribe\u001b[0m:\u001b[36m139\u001b[0m - \u001b[1mTranscribing harvard.wav with mlx backend\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added 22 chunks\n",
      "Processing: harvard.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-02 21:28:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36mtranscribe\u001b[0m:\u001b[36m193\u001b[0m - \u001b[1mTranscription complete in 3.65s: 216 chars, 6 segments\u001b[0m\n",
      "\u001b[32m2026-01-02 21:28:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36madd_chunks\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mAdding 6 chunks to ChromaDB (batch_size=16)\u001b[0m\n",
      "\u001b[32m2026-01-02 21:28:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36madd_chunks\u001b[0m:\u001b[36m147\u001b[0m - \u001b[1mSuccessfully added 6 chunks in 1 batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added 6 audio segments\n"
     ]
    }
   ],
   "source": [
    "# Process documents\n",
    "doc_path = Path(\"../test_documents/2112.13734v2.pdf\")\n",
    "if doc_path.exists():\n",
    "    pipeline.process_document(doc_path)\n",
    "\n",
    "# Process audio\n",
    "audio_path = Path(\"../test_documents/harvard.wav\")\n",
    "if audio_path.exists():\n",
    "    pipeline.process_audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Modal Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d2681bfa764300af7fd8a747f9b894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1 [AUDIO]:\n",
      "  Source: harvard.wav\n",
      "  Text: A salt pickle tastes fine with ham....\n",
      "\n",
      "Result 2 [AUDIO]:\n",
      "  Source: harvard.wav\n",
      "  Text: A cold dip restores health and zest....\n",
      "\n",
      "Result 3 [AUDIO]:\n",
      "  Source: harvard.wav\n",
      "  Text: It takes heat to bring out the odor....\n"
     ]
    }
   ],
   "source": [
    "results = pipeline.search(\"How does the salt pickle taste?\", top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    modality = result['metadata'].get('modality', 'unknown')\n",
    "    source = result['metadata'].get('source', 'unknown')\n",
    "    \n",
    "    print(f\"\\nResult {i} [{modality.upper()}]:\")\n",
    "    print(f\"  Source: {source}\")\n",
    "    print(f\"  Text: {result['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a702ccbee2c45c1b29ae3e8bbf99ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1 [DOCUMENT]:\n",
      "  Source: 2112.13734v2.pdf\n",
      "  Text: Learning models that generalize under different distribution shifts in medical imaging has been a lo...\n",
      "\n",
      "Result 2 [DOCUMENT]:\n",
      "  Source: 2112.13734v2.pdf\n",
      "  Text: We aim to classify 4 chest X-ray pathologies, namely Cardiomegaly, Consolidation, Edema, and Effusio...\n",
      "\n",
      "Result 3 [DOCUMENT]:\n",
      "  Source: 2112.13734v2.pdf\n",
      "  Text: - [1] A. Bustos, A. Pertusa, J. M. Salinas, and M. Iglesia-Vayá. Padchest: A large chest x-ray image...\n"
     ]
    }
   ],
   "source": [
    "results = pipeline.search(\"medical imaging\", top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    modality = result['metadata'].get('modality', 'unknown')\n",
    "    source = result['metadata'].get('source', 'unknown')\n",
    "    \n",
    "    print(f\"\\nResult {i} [{modality.upper()}]:\")\n",
    "    print(f\"  Source: {source}\")\n",
    "    print(f\"  Text: {result['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Demonstrated:\n",
    "- Unified multimodal pipeline\n",
    "- Document and audio processing\n",
    "- Cross-modal search\n",
    "\n",
    "See documentation for more examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
