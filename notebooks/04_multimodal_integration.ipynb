{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Integration\n",
    "\n",
    "Combine documents and audio in a unified pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from vertector_data_ingestion import (\n",
    "    UniversalConverter,\n",
    "    LocalMpsConfig,\n",
    "    HybridChunker,\n",
    "    ChromaAdapter,\n",
    "    create_audio_transcriber,\n",
    "    AudioConfig,\n",
    "    WhisperModelSize,\n",
    "    setup_logging,\n",
    ")\n",
    "\n",
    "from vertector_data_ingestion.models.config import ChunkingConfig\n",
    "\n",
    "setup_logging(log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalPipeline:\n",
    "    def __init__(self):\n",
    "        self.converter = UniversalConverter(LocalMpsConfig())\n",
    "        self.chunk_config = ChunkingConfig(\n",
    "            tokenizer=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "            max_tokens=512,\n",
    "        )\n",
    "        self.chunker = HybridChunker(config=self.chunk_config)\n",
    "        self.audio_transcriber = create_audio_transcriber(\n",
    "            AudioConfig(model_size=WhisperModelSize.BASE)\n",
    "        )\n",
    "        self.vector_store = ChromaAdapter(\n",
    "            collection_name=\"multimodal\",\n",
    "            embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n",
    "        )\n",
    "    \n",
    "    def process_document(self, path: Path):\n",
    "        print(f\"Processing: {path.name}\")\n",
    "        doc = self.converter.convert(path)\n",
    "        chunks = self.chunker.chunk_document(doc)\n",
    "        \n",
    "        for chunk in chunks.chunks:\n",
    "            chunk.metadata[\"modality\"] = \"document\"\n",
    "            chunk.metadata[\"source\"] = path.name\n",
    "        \n",
    "        self.vector_store.add_chunks(chunks.chunks)\n",
    "        print(f\"  Added {len(chunks.chunks)} chunks\")\n",
    "        return len(chunks.chunks)\n",
    "    \n",
    "    def process_audio(self, path: Path):\n",
    "        print(f\"Processing: {path.name}\")\n",
    "        result = self.audio_transcriber.transcribe(path)\n",
    "        \n",
    "        from vertector_data_ingestion.models.chunk import DocumentChunk\n",
    "        from transformers import AutoTokenizer\n",
    "        \n",
    "        # Load tokenizer for token counting\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.chunk_config.tokenizer)\n",
    "        \n",
    "        chunks = []\n",
    "        for i, segment in enumerate(result.segments):\n",
    "            # Count tokens in the segment text\n",
    "            tokens = tokenizer.encode(segment.text, add_special_tokens=False)\n",
    "            \n",
    "            chunk = DocumentChunk(\n",
    "                chunk_id=f\"{path.stem}_{i}\",\n",
    "                text=segment.text,\n",
    "                token_count=len(tokens),\n",
    "                source_path=path,\n",
    "                chunk_index=i,\n",
    "                metadata={\n",
    "                    \"modality\": \"audio\",\n",
    "                    \"source\": path.name,\n",
    "                    \"start_time\": segment.start,\n",
    "                    \"end_time\": segment.end,\n",
    "                    \"duration\": segment.end - segment.start,\n",
    "                }\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        self.vector_store.add_chunks(chunks)\n",
    "        print(f\"  Added {len(chunks)} audio segments\")\n",
    "        return len(chunks)\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5):\n",
    "        return self.vector_store.search(query, top_k=top_k)\n",
    "\n",
    "pipeline = MultimodalPipeline()\n",
    "print(\"âœ“ Pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Documents and Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents\n",
    "doc_path = Path(\"../test_documents/2112.13734v2.pdf\")\n",
    "if doc_path.exists():\n",
    "    pipeline.process_document(doc_path)\n",
    "\n",
    "# Process audio\n",
    "audio_path = Path(\"../test_documents/harvard.wav\")\n",
    "if audio_path.exists():\n",
    "    pipeline.process_audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Modal Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipeline.search(\"How does the salt pickle taste?\", top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    modality = result['metadata'].get('modality', 'unknown')\n",
    "    source = result['metadata'].get('source', 'unknown')\n",
    "    \n",
    "    print(f\"\\nResult {i} [{modality.upper()}]:\")\n",
    "    print(f\"  Source: {source}\")\n",
    "    print(f\"  Text: {result['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipeline.search(\"medical imaging\", top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    modality = result['metadata'].get('modality', 'unknown')\n",
    "    source = result['metadata'].get('source', 'unknown')\n",
    "    \n",
    "    print(f\"\\nResult {i} [{modality.upper()}]:\")\n",
    "    print(f\"  Source: {source}\")\n",
    "    print(f\"  Text: {result['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Demonstrated:\n",
    "- Unified multimodal pipeline\n",
    "- Document and audio processing\n",
    "- Cross-modal search\n",
    "\n",
    "See documentation for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
