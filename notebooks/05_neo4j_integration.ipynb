{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j SimpleKGPipeline Integration\n",
    "\n",
    "This notebook demonstrates the complete Neo4j integration with Vertector, showcasing:\n",
    "- Document loading and chunking with rich metadata\n",
    "- Audio transcription and segment-based chunking\n",
    "- Multimodal pipeline handling both documents and audio\n",
    "- State isolation between different modalities\n",
    "- Metadata preservation through the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-09 14:42:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.monitoring.logger\u001b[0m:\u001b[36msetup_logging\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mLogging initialized at INFO level\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Neo4j integration components imported successfully\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from vertector_data_ingestion import setup_logging\n",
    "from vertector_data_ingestion.integrations.neo4j import (\n",
    "    MultimodalLoader,\n",
    "    VertectorAudioLoader,\n",
    "    VertectorDataLoader,\n",
    "    VertectorTextSplitter,\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(log_level=\"INFO\")\n",
    "\n",
    "print(\"✓ Neo4j integration components imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Document Processing\n",
    "\n",
    "Load and chunk a PDF document using Docling's structure-aware chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-09 14:43:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-09 14:43:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-09 14:43:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mHardware detected: mps\u001b[0m\n",
      "\u001b[32m2026-01-09 14:43:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mInitialized UniversalConverter on mps\u001b[0m\n",
      "\u001b[32m2026-01-09 14:43:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_ensure_models_available\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mChecking model availability...\u001b[0m\n",
      "\u001b[32m2026-01-09 14:43:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading tokenizer: Qwen/Qwen3-Embedding-0.6B (padding_side=left)\u001b[0m\n",
      "\u001b[32m2026-01-09 14:43:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mInitialized HybridChunker with max_tokens=512, merge_peers=True\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document: sample.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-09 14:43:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mUsing Classic pipeline (default) for sample.pdf\u001b[0m\n",
      "\u001b[32m2026-01-09 14:43:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting sample.pdf with classic pipeline\u001b[0m\n",
      "2026-01-09 14:43:46,600 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-09 14:43:46,667 - INFO - Going to convert document batch...\n",
      "2026-01-09 14:43:46,668 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-09 14:43:46,698 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-09 14:43:46,700 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-09 14:43:46,712 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-09 14:43:46,718 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 14:43:47,658 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-09 14:43:47,663 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-09 14:43:47,671 - INFO - Accelerator device: 'mps'\n",
      "2026-01-09 14:43:48,699 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-09 14:43:48,702 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-09 14:43:51,380 - INFO - Accelerator device: 'mps'\n",
      "2026-01-09 14:43:52,382 - INFO - Processing document sample.pdf\n",
      "2026-01-09 14:43:54,055 - INFO - Finished converting document sample.pdf in 7.46 sec.\n",
      "\u001b[32m2026-01-09 14:43:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted sample.pdf in 10.97s (1 pages, 0.1 pages/sec)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Document loaded:\n",
      "  Type: document\n",
      "  Pages: 1\n",
      "  Processing time: 10.973098039627075s\n",
      "  Text length: 2713 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize document loader and splitter\n",
    "doc_loader = VertectorDataLoader()\n",
    "doc_splitter = VertectorTextSplitter(loader=doc_loader, chunk_size=512)\n",
    "\n",
    "# Load document\n",
    "pdf_path = Path(\"../test_documents/sample.pdf\")\n",
    "print(f\"Loading document: {pdf_path.name}\")\n",
    "\n",
    "doc_result = await doc_loader.run(pdf_path)\n",
    "\n",
    "print(f\"\\n✓ Document loaded:\")\n",
    "print(f\"  Type: {doc_result.document_info.document_type}\")\n",
    "print(f\"  Pages: {doc_result.document_info.metadata['num_pages']}\")\n",
    "print(f\"  Processing time: {doc_result.document_info.metadata['processing_time']}s\")\n",
    "print(f\"  Text length: {len(doc_result.text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Multi-Domain Balanced Sampling Improves Out-of-Distribution Generalization of Chest X-ray Pathology Prediction Models\n",
      "\n",
      "Enoch Tetteh Mila, Quebec AI Institute AMMI, AIMS Rwanda etetteh@aimsammi.org\n",
      "\n",
      "Joseph Viviano Mila, Quebec AI Institute University of\n",
      "\n",
      "David Krueger Mila, Quebec AI Institute\n",
      "\n",
      "University of Montreal University of Cambridge\n",
      "\n",
      "Yoshua Bengio Mila, Quebec AI Institute University of Montreal\n",
      "\n",
      "Joseph Paul Cohen Mila, Quebec AI Institute AIMI, Stanford University\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Learning models that generalize under different distribution shifts in medical imaging has been a long-standing research challenge. There have been several proposals for efficient and robust visual representation learning among vision research practitioners, especially in the sensitive and critical biomedical domain. In this paper, we propose an idea for out-of-distribution generalization of chest X-ray pathologies that uses a simple balanced batch sampling technique. We observed that balanced sampling between the multiple training datasets improves the performance over baseline models trained without balancing. Code for this work is available on Github. 1\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Pathology detection or classification in medical imaging using deep learning [5], [4] continues to be an open research challenge. Although the field of computer vision has progressed significantly owing to advances in model architecture, optimization techniques, and data augmentation, learning relevant feature representations from visual data still remains a challenge [2] [9]. The field of medical imaging using deep learning suffers from the same issues, since they rely mostly on the same deep learning approaches inspired by studies on general purpose datasets, like ImageNet [13]. The issue becomes more challenging when test data is subject to distribution shifts [12]. When confronted with chest X-rays from different datasets, a straightforward approach, followed by [2], is to merge these datasets and form mini-batches by sampling uniformly from the merged dataset.\n",
      "\n",
      "The question is: can we find a more robust and efficient way of learning representations from medical images by accounting for which dataset each example came from?\n",
      "\n",
      "In this work, we examine how balanced batch sampling from each training dataset can improve a model's generalization on out-of-distribution (OoD) chest X-ray datasets. We compare this to [2] and a baseline model trained without balanced batching.\n",
      "\n",
      "Our work shows that, by training vision algorithms on chest X-rays using balanced mini-batches, we may achieve performance gains during inference on out-of-distribution chest X-ray datasets.\n",
      "\n",
      "1 https://github.com/etetteh/OoD\\_Gen-Chest\\_Xray\n"
     ]
    }
   ],
   "source": [
    "print(doc_result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the document\n",
    "print(\"Chunking document with structure-aware HybridChunker...\")\n",
    "doc_chunks = await doc_splitter.run(doc_result.text)\n",
    "\n",
    "print(f\"\\n✓ Created {len(doc_chunks.chunks)} document chunks\")\n",
    "print(f\"\\nFirst chunk:\")\n",
    "print(f\"  Text: {doc_chunks.chunks[0].text[:100]}...\")\n",
    "print(f\"  Index: {doc_chunks.chunks[0].index}\")\n",
    "print(f\"  Metadata keys: {list(doc_chunks.chunks[0].metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine rich metadata from multiple chunks\n",
    "print(\"Document chunk metadata examples:\\n\")\n",
    "\n",
    "for i in range(min(3, len(doc_chunks.chunks))):\n",
    "    chunk = doc_chunks.chunks[i]\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Token count: {chunk.metadata.get('token_count')}\")\n",
    "    print(f\"  Page number: {chunk.metadata.get('page_no', 'N/A')}\")\n",
    "    print(f\"  Section: {chunk.metadata.get('subsection_path', 'N/A')[:80]}\")\n",
    "    print(f\"  Is table: {chunk.metadata.get('is_table', 'False')}\")\n",
    "    print(f\"  Is heading: {chunk.metadata.get('is_heading', 'False')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Audio Processing\n",
    "\n",
    "Transcribe and chunk audio using Whisper segments with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize audio loader and splitter\n",
    "audio_loader = VertectorAudioLoader()\n",
    "audio_splitter = VertectorTextSplitter(loader=audio_loader, chunk_size=512)\n",
    "\n",
    "# Load audio\n",
    "audio_path = Path(\"../test_documents/harvard.wav\")\n",
    "print(f\"Loading audio: {audio_path.name}\")\n",
    "\n",
    "audio_result = await audio_loader.run(audio_path)\n",
    "\n",
    "print(f\"\\n✓ Audio loaded:\")\n",
    "print(f\"  Type: {audio_result.document_info.document_type}\")\n",
    "print(f\"  Duration: {audio_result.document_info.metadata['duration']}s\")\n",
    "print(f\"  Language: {audio_result.document_info.metadata['language']}\")\n",
    "print(f\"  Segments: {audio_result.document_info.metadata['segments']}\")\n",
    "print(f\"  Model: {audio_result.document_info.metadata['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the audio\n",
    "print(\"Chunking audio using Whisper segments...\")\n",
    "audio_chunks = await audio_splitter.run(audio_result.text)\n",
    "\n",
    "print(f\"\\n✓ Created {len(audio_chunks.chunks)} audio chunks\")\n",
    "print(f\"\\nFirst chunk:\")\n",
    "print(f\"  Text: {audio_chunks.chunks[0].text}\")\n",
    "print(f\"  Index: {audio_chunks.chunks[0].index}\")\n",
    "print(f\"  Metadata keys: {list(audio_chunks.chunks[0].metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine audio chunk metadata with timestamps\n",
    "print(\"Audio chunk metadata (with timestamps):\\n\")\n",
    "\n",
    "for i, chunk in enumerate(audio_chunks.chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Text: {chunk.text}\")\n",
    "    print(f\"  Start: {chunk.metadata['start_time']}s\")\n",
    "    print(f\"  End: {chunk.metadata['end_time']}s\")\n",
    "    print(f\"  Duration: {chunk.metadata['duration']}s\")\n",
    "    print(f\"  Token count: {chunk.metadata['token_count']}\")\n",
    "    print(f\"  Document ID: {chunk.metadata['document_id']}\")\n",
    "    print(f\"  Chunk ID: {chunk.metadata['chunk_id']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multimodal Pipeline\n",
    "\n",
    "Use a single MultimodalLoader to handle both documents and audio, with automatic modality detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multimodal loader and splitter\n",
    "multimodal_loader = MultimodalLoader()\n",
    "multimodal_splitter = VertectorTextSplitter(loader=multimodal_loader, chunk_size=512)\n",
    "\n",
    "print(\"✓ Multimodal pipeline initialized\")\n",
    "print(\"  Supports: PDF, DOCX, PPTX, XLSX, WAV, MP3, M4A, FLAC, OGG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Document → Audio (State Isolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk document first\n",
    "print(\"Step 1: Processing document...\")\n",
    "doc_result = await multimodal_loader.run(pdf_path)\n",
    "doc_chunks = await multimodal_splitter.run(doc_result.text)\n",
    "\n",
    "print(f\"✓ Document: {len(doc_chunks.chunks)} chunks\")\n",
    "print(f\"  First chunk text: {doc_chunks.chunks[0].text[:80]}...\")\n",
    "print(f\"  Has page_no: {'page_no' in doc_chunks.chunks[0].metadata}\")\n",
    "print(f\"  Has start_time: {'start_time' in doc_chunks.chunks[0].metadata}\")\n",
    "\n",
    "# Now load and chunk audio (state should be isolated)\n",
    "print(\"\\nStep 2: Processing audio (after document)...\")\n",
    "audio_result = await multimodal_loader.run(audio_path)\n",
    "audio_chunks = await multimodal_splitter.run(audio_result.text)\n",
    "\n",
    "print(f\"✓ Audio: {len(audio_chunks.chunks)} chunks\")\n",
    "print(f\"  First chunk text: {audio_chunks.chunks[0].text[:80]}...\")\n",
    "print(f\"  Has page_no: {'page_no' in audio_chunks.chunks[0].metadata}\")\n",
    "print(f\"  Has start_time: {'start_time' in audio_chunks.chunks[0].metadata}\")\n",
    "\n",
    "# Verify state isolation\n",
    "print(\"\\n✓ State Isolation Verified:\")\n",
    "print(f\"  Document chunks != Audio chunks: {doc_chunks.chunks[0].text != audio_chunks.chunks[0].text}\")\n",
    "print(f\"  loader.last_document is None: {multimodal_loader.last_document is None}\")\n",
    "print(f\"  loader.last_transcription_result is not None: {multimodal_loader.last_transcription_result is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Audio → Document (Reverse Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh loader\n",
    "fresh_loader = MultimodalLoader()\n",
    "fresh_splitter = VertectorTextSplitter(loader=fresh_loader, chunk_size=512)\n",
    "\n",
    "# Load audio first\n",
    "print(\"Step 1: Processing audio...\")\n",
    "audio_result = await fresh_loader.run(audio_path)\n",
    "audio_chunks = await fresh_splitter.run(audio_result.text)\n",
    "\n",
    "print(f\"✓ Audio: {len(audio_chunks.chunks)} chunks\")\n",
    "print(f\"  Modality: {audio_chunks.chunks[0].metadata.get('modality', 'N/A')}\")\n",
    "\n",
    "# Then load document\n",
    "print(\"\\nStep 2: Processing document (after audio)...\")\n",
    "doc_result = await fresh_loader.run(pdf_path)\n",
    "doc_chunks = await fresh_splitter.run(doc_result.text)\n",
    "\n",
    "print(f\"✓ Document: {len(doc_chunks.chunks)} chunks\")\n",
    "print(f\"  Has page_no: {'page_no' in doc_chunks.chunks[0].metadata}\")\n",
    "\n",
    "# Verify state isolation\n",
    "print(\"\\n✓ Reverse State Isolation Verified:\")\n",
    "print(f\"  Audio chunks != Document chunks: {audio_chunks.chunks[0].text != doc_chunks.chunks[0].text}\")\n",
    "print(f\"  loader.last_document is not None: {fresh_loader.last_document is not None}\")\n",
    "print(f\"  loader.last_transcription_result is None: {fresh_loader.last_transcription_result is None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Metadata Comparison\n",
    "\n",
    "Compare the metadata between document and audio chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': [\n",
    "        'chunk_id',\n",
    "        'document_id', \n",
    "        'token_count',\n",
    "        'page_no',\n",
    "        'section_title',\n",
    "        'subsection_path',\n",
    "        'is_table',\n",
    "        'is_heading',\n",
    "        'bbox',\n",
    "        'modality',\n",
    "        'start_time',\n",
    "        'end_time',\n",
    "        'duration',\n",
    "        'language'\n",
    "    ],\n",
    "    'Document Chunks': [\n",
    "        '✓' if 'chunk_id' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'document_id' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'token_count' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'page_no' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'section_title' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'subsection_path' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_table' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_heading' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'bbox' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'modality' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'start_time' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'end_time' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'duration' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'language' in doc_chunks.chunks[0].metadata else '✗',\n",
    "    ],\n",
    "    'Audio Chunks': [\n",
    "        '✓' if 'chunk_id' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'document_id' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'token_count' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'page_no' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'section_title' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'subsection_path' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_table' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_heading' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'bbox' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'modality' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'start_time' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'end_time' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'duration' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'language' in audio_chunks.chunks[0].metadata else '✗',\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Metadata Feature Comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✅ **Document Processing**\n",
    "- Structure-aware chunking with Docling HybridChunker\n",
    "- Rich metadata: page numbers, sections, bounding boxes, table detection\n",
    "\n",
    "✅ **Audio Processing**  \n",
    "- Whisper transcription with MLX acceleration\n",
    "- Segment-based chunking with timestamps\n",
    "- Audio metadata: start_time, end_time, duration, language\n",
    "\n",
    "✅ **Multimodal Pipeline**\n",
    "- Single loader handles both documents and audio\n",
    "- Automatic modality detection by file extension\n",
    "- Proper state isolation between modalities\n",
    "\n",
    "✅ **Neo4j Integration**\n",
    "- Compatible with Neo4j SimpleKGPipeline\n",
    "- Preserves rich metadata for knowledge graph construction\n",
    "- Ready for production use\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Property Delegation**: MultimodalLoader properly exposes sub-loader state\n",
    "2. **State Isolation**: Loading one modality clears the state of the other\n",
    "3. **Metadata Preservation**: All rich metadata flows through to Neo4j chunks\n",
    "4. **Document IDs**: Proper document_id, chunk_id for both documents and audio\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Connect to Neo4j database\n",
    "- Define entity and relation schemas\n",
    "- Build knowledge graph from multimodal data\n",
    "- Query and visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
