{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j SimpleKGPipeline Integration\n",
    "\n",
    "This notebook demonstrates the complete Neo4j integration with Vertector, showcasing:\n",
    "- Document loading and chunking with rich metadata\n",
    "- Audio transcription and segment-based chunking\n",
    "- Multimodal pipeline handling both documents and audio\n",
    "- State isolation between different modalities\n",
    "- Metadata preservation through the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 16:22:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.monitoring.logger\u001b[0m:\u001b[36msetup_logging\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mLogging initialized at INFO level\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Neo4j integration components imported successfully\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from vertector_data_ingestion import setup_logging\n",
    "from vertector_data_ingestion.integrations.neo4j import (\n",
    "    MultimodalLoader,\n",
    "    VertectorAudioLoader,\n",
    "    VertectorDataLoader,\n",
    "    VertectorTextSplitter,\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(log_level=\"INFO\")\n",
    "\n",
    "print(\"✓ Neo4j integration components imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Document Processing\n",
    "\n",
    "Load and chunk a PDF document using Docling's structure-aware chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 16:22:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-03 16:22:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-03 16:22:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mHardware detected: mps\u001b[0m\n",
      "\u001b[32m2026-01-03 16:22:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mInitialized UniversalConverter on mps\u001b[0m\n",
      "\u001b[32m2026-01-03 16:22:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_ensure_models_available\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mChecking model availability...\u001b[0m\n",
      "\u001b[32m2026-01-03 16:22:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading tokenizer: Qwen/Qwen3-Embedding-0.6B (padding_side=left)\u001b[0m\n",
      "\u001b[32m2026-01-03 16:22:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mInitialized HybridChunker with max_tokens=512, merge_peers=True\u001b[0m\n",
      "\u001b[32m2026-01-03 16:22:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mUsing Classic pipeline (default) for 2112.13734v2.pdf\u001b[0m\n",
      "\u001b[32m2026-01-03 16:22:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting 2112.13734v2.pdf with classic pipeline\u001b[0m\n",
      "2026-01-03 16:22:54,689 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-03 16:22:54,733 - INFO - Going to convert document batch...\n",
      "2026-01-03 16:22:54,734 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document: 2112.13734v2.pdf\n",
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 16:22:54,755 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-03 16:22:54,757 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-03 16:22:54,765 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-03 16:22:54,771 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-03 16:22:55,285 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-03 16:22:55,288 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-03 16:22:55,294 - INFO - Accelerator device: 'mps'\n",
      "2026-01-03 16:22:56,103 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-03 16:22:56,104 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-03 16:22:56,332 - INFO - Accelerator device: 'mps'\n",
      "2026-01-03 16:22:57,159 - INFO - Processing document 2112.13734v2.pdf\n",
      "2026-01-03 16:23:02,772 - INFO - Finished converting document 2112.13734v2.pdf in 8.08 sec.\n",
      "\u001b[32m2026-01-03 16:23:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted 2112.13734v2.pdf in 8.24s (4 pages, 0.5 pages/sec)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Document loaded:\n",
      "  Type: document\n",
      "  Pages: 4\n",
      "  Processing time: 8.23647403717041s\n",
      "  Text length: 17923 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize document loader and splitter\n",
    "doc_loader = VertectorDataLoader()\n",
    "doc_splitter = VertectorTextSplitter(loader=doc_loader, chunk_size=512)\n",
    "\n",
    "# Load document\n",
    "pdf_path = Path(\"../test_documents/2112.13734v2.pdf\")\n",
    "print(f\"Loading document: {pdf_path.name}\")\n",
    "\n",
    "doc_result = await doc_loader.run(pdf_path)\n",
    "\n",
    "print(f\"\\n✓ Document loaded:\")\n",
    "print(f\"  Type: {doc_result.document_info.document_type}\")\n",
    "print(f\"  Pages: {doc_result.document_info.metadata['num_pages']}\")\n",
    "print(f\"  Processing time: {doc_result.document_info.metadata['processing_time']}s\")\n",
    "print(f\"  Text length: {len(doc_result.text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 16:51:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mChunking document: 2112.13734v2.pdf (4 pages)\u001b[0m\n",
      "\u001b[32m2026-01-03 16:51:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mCreated 22 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking document with structure-aware HybridChunker...\n",
      "\n",
      "✓ Created 22 document chunks\n",
      "\n",
      "First chunk:\n",
      "  Text: Enoch Tetteh Mila, Quebec AI Institute AMMI, AIMS Rwanda etetteh@aimsammi.org\n",
      "Joseph Viviano Mila, Q...\n",
      "  Index: 0\n",
      "  Metadata keys: ['chunk_id', 'token_count', 'document_id', 'page_no', 'subsection_path', 'bbox']\n"
     ]
    }
   ],
   "source": [
    "# Chunk the document\n",
    "print(\"Chunking document with structure-aware HybridChunker...\")\n",
    "doc_chunks = await doc_splitter.run(doc_result.text)\n",
    "\n",
    "print(f\"\\n✓ Created {len(doc_chunks.chunks)} document chunks\")\n",
    "print(f\"\\nFirst chunk:\")\n",
    "print(f\"  Text: {doc_chunks.chunks[0].text[:100]}...\")\n",
    "print(f\"  Index: {doc_chunks.chunks[0].index}\")\n",
    "print(f\"  Metadata keys: {list(doc_chunks.chunks[0].metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine rich metadata from multiple chunks\n",
    "print(\"Document chunk metadata examples:\\n\")\n",
    "\n",
    "for i in range(min(3, len(doc_chunks.chunks))):\n",
    "    chunk = doc_chunks.chunks[i]\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Token count: {chunk.metadata.get('token_count')}\")\n",
    "    print(f\"  Page number: {chunk.metadata.get('page_no', 'N/A')}\")\n",
    "    print(f\"  Section: {chunk.metadata.get('subsection_path', 'N/A')[:80]}\")\n",
    "    print(f\"  Is table: {chunk.metadata.get('is_table', 'False')}\")\n",
    "    print(f\"  Is heading: {chunk.metadata.get('is_heading', 'False')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Audio Processing\n",
    "\n",
    "Transcribe and chunk audio using Whisper segments with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 16:53:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.audio_factory\u001b[0m:\u001b[36mcreate_audio_transcriber\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mCreating audio transcriber: model=base, backend=auto\u001b[0m\n",
      "\u001b[32m2026-01-03 16:53:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-03 16:53:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mInitializing WhisperTranscriber with model=base, device=mlx, backend=auto\u001b[0m\n",
      "\u001b[32m2026-01-03 16:53:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading tokenizer: Qwen/Qwen3-Embedding-0.6B (padding_side=left)\u001b[0m\n",
      "\u001b[32m2026-01-03 16:53:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mInitialized HybridChunker with max_tokens=512, merge_peers=True\u001b[0m\n",
      "\u001b[32m2026-01-03 16:53:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m_load_model\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mLoading Whisper model: base\u001b[0m\n",
      "\u001b[32m2026-01-03 16:53:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m_load_model\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mLoaded MLX Whisper model: base\u001b[0m\n",
      "\u001b[32m2026-01-03 16:53:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36mtranscribe\u001b[0m:\u001b[36m139\u001b[0m - \u001b[1mTranscribing harvard.wav with mlx backend\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio: harvard.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-03 16:53:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36mtranscribe\u001b[0m:\u001b[36m193\u001b[0m - \u001b[1mTranscription complete in 1.93s: 216 chars, 6 segments\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Audio loaded:\n",
      "  Type: audio\n",
      "  Duration: 1.9336051940917969s\n",
      "  Language: en\n",
      "  Segments: 6\n",
      "  Model: whisper-base-mlx\n"
     ]
    }
   ],
   "source": [
    "# Initialize audio loader and splitter\n",
    "audio_loader = VertectorAudioLoader()\n",
    "audio_splitter = VertectorTextSplitter(loader=audio_loader, chunk_size=512)\n",
    "\n",
    "# Load audio\n",
    "audio_path = Path(\"../test_documents/harvard.wav\")\n",
    "print(f\"Loading audio: {audio_path.name}\")\n",
    "\n",
    "audio_result = await audio_loader.run(audio_path)\n",
    "\n",
    "print(f\"\\n✓ Audio loaded:\")\n",
    "print(f\"  Type: {audio_result.document_info.document_type}\")\n",
    "print(f\"  Duration: {audio_result.document_info.metadata['duration']}s\")\n",
    "print(f\"  Language: {audio_result.document_info.metadata['language']}\")\n",
    "print(f\"  Segments: {audio_result.document_info.metadata['segments']}\")\n",
    "print(f\"  Model: {audio_result.document_info.metadata['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking audio using Whisper segments...\n",
      "\n",
      "✓ Created 6 audio chunks\n",
      "\n",
      "First chunk:\n",
      "  Text: The stale smell of old beer lingers.\n",
      "  Index: 0\n",
      "  Metadata keys: ['chunk_id', 'token_count', 'document_id', 'modality', 'start_time', 'end_time', 'duration', 'language']\n"
     ]
    }
   ],
   "source": [
    "# Chunk the audio\n",
    "print(\"Chunking audio using Whisper segments...\")\n",
    "audio_chunks = await audio_splitter.run(audio_result.text)\n",
    "\n",
    "print(f\"\\n✓ Created {len(audio_chunks.chunks)} audio chunks\")\n",
    "print(f\"\\nFirst chunk:\")\n",
    "print(f\"  Text: {audio_chunks.chunks[0].text}\")\n",
    "print(f\"  Index: {audio_chunks.chunks[0].index}\")\n",
    "print(f\"  Metadata keys: {list(audio_chunks.chunks[0].metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio chunk metadata (with timestamps):\n",
      "\n",
      "Chunk 0:\n",
      "  Text: The stale smell of old beer lingers.\n",
      "  Start: 0.8600000000000003s\n",
      "  End: 3.64s\n",
      "  Duration: 2.78s\n",
      "  Token count: 9\n",
      "  Document ID: harvard\n",
      "  Chunk ID: harvard_0\n",
      "\n",
      "Chunk 1:\n",
      "  Text: It takes heat to bring out the odor.\n",
      "  Start: 4.18s\n",
      "  End: 6.18s\n",
      "  Duration: 2.0s\n",
      "  Token count: 9\n",
      "  Document ID: harvard\n",
      "  Chunk ID: harvard_1\n",
      "\n",
      "Chunk 2:\n",
      "  Text: A cold dip restores health and zest.\n",
      "  Start: 7.02s\n",
      "  End: 9.16s\n",
      "  Duration: 2.1400000000000006s\n",
      "  Token count: 8\n",
      "  Document ID: harvard\n",
      "  Chunk ID: harvard_2\n",
      "\n",
      "Chunk 3:\n",
      "  Text: A salt pickle tastes fine with ham.\n",
      "  Start: 9.96s\n",
      "  End: 12.0s\n",
      "  Duration: 2.039999999999999s\n",
      "  Token count: 8\n",
      "  Document ID: harvard\n",
      "  Chunk ID: harvard_3\n",
      "\n",
      "Chunk 4:\n",
      "  Text: Tacos al pastor are my favorite.\n",
      "  Start: 12.68s\n",
      "  End: 14.32s\n",
      "  Duration: 1.6400000000000006s\n",
      "  Token count: 8\n",
      "  Document ID: harvard\n",
      "  Chunk ID: harvard_4\n",
      "\n",
      "Chunk 5:\n",
      "  Text: A zestful food is the hot cross bun.\n",
      "  Start: 15.12s\n",
      "  End: 17.42s\n",
      "  Duration: 2.3000000000000025s\n",
      "  Token count: 10\n",
      "  Document ID: harvard\n",
      "  Chunk ID: harvard_5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine audio chunk metadata with timestamps\n",
    "print(\"Audio chunk metadata (with timestamps):\\n\")\n",
    "\n",
    "for i, chunk in enumerate(audio_chunks.chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Text: {chunk.text}\")\n",
    "    print(f\"  Start: {chunk.metadata['start_time']}s\")\n",
    "    print(f\"  End: {chunk.metadata['end_time']}s\")\n",
    "    print(f\"  Duration: {chunk.metadata['duration']}s\")\n",
    "    print(f\"  Token count: {chunk.metadata['token_count']}\")\n",
    "    print(f\"  Document ID: {chunk.metadata['document_id']}\")\n",
    "    print(f\"  Chunk ID: {chunk.metadata['chunk_id']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multimodal Pipeline\n",
    "\n",
    "Use a single MultimodalLoader to handle both documents and audio, with automatic modality detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 16:56:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-03 16:56:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-03 16:56:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mHardware detected: mps\u001b[0m\n",
      "\u001b[32m2026-01-03 16:56:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mInitialized UniversalConverter on mps\u001b[0m\n",
      "\u001b[32m2026-01-03 16:56:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_ensure_models_available\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mChecking model availability...\u001b[0m\n",
      "\u001b[32m2026-01-03 16:56:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.audio_factory\u001b[0m:\u001b[36mcreate_audio_transcriber\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mCreating audio transcriber: model=base, backend=auto\u001b[0m\n",
      "\u001b[32m2026-01-03 16:56:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-03 16:56:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mInitializing WhisperTranscriber with model=base, device=mlx, backend=auto\u001b[0m\n",
      "\u001b[32m2026-01-03 16:56:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading tokenizer: Qwen/Qwen3-Embedding-0.6B (padding_side=left)\u001b[0m\n",
      "\u001b[32m2026-01-03 16:56:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mInitialized HybridChunker with max_tokens=512, merge_peers=True\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Multimodal pipeline initialized\n",
      "  Supports: PDF, DOCX, PPTX, XLSX, WAV, MP3, M4A, FLAC, OGG\n"
     ]
    }
   ],
   "source": [
    "# Initialize multimodal loader and splitter\n",
    "multimodal_loader = MultimodalLoader()\n",
    "multimodal_splitter = VertectorTextSplitter(loader=multimodal_loader, chunk_size=512)\n",
    "\n",
    "print(\"✓ Multimodal pipeline initialized\")\n",
    "print(\"  Supports: PDF, DOCX, PPTX, XLSX, WAV, MP3, M4A, FLAC, OGG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Document → Audio (State Isolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 16:57:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mUsing Classic pipeline (default) for 2112.13734v2.pdf\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting 2112.13734v2.pdf with classic pipeline\u001b[0m\n",
      "2026-01-03 16:57:11,588 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-03 16:57:11,592 - INFO - Going to convert document batch...\n",
      "2026-01-03 16:57:11,594 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-03 16:57:11,594 - INFO - Accelerator device: 'mps'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Processing document...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 16:57:12,391 - INFO - Accelerator device: 'mps'\n",
      "2026-01-03 16:57:13,216 - INFO - Processing document 2112.13734v2.pdf\n",
      "2026-01-03 16:57:18,891 - INFO - Finished converting document 2112.13734v2.pdf in 7.30 sec.\n",
      "\u001b[32m2026-01-03 16:57:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted 2112.13734v2.pdf in 7.43s (4 pages, 0.5 pages/sec)\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mChunking document: 2112.13734v2.pdf (4 pages)\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mCreated 22 chunks\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m_load_model\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mLoading Whisper model: base\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m_load_model\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mLoaded MLX Whisper model: base\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36mtranscribe\u001b[0m:\u001b[36m139\u001b[0m - \u001b[1mTranscribing harvard.wav with mlx backend\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Document: 22 chunks\n",
      "  First chunk text: Enoch Tetteh Mila, Quebec AI Institute AMMI, AIMS Rwanda etetteh@aimsammi.org\n",
      "Jo...\n",
      "  Has page_no: True\n",
      "  Has start_time: False\n",
      "\n",
      "Step 2: Processing audio (after document)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 16:57:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36mtranscribe\u001b[0m:\u001b[36m193\u001b[0m - \u001b[1mTranscription complete in 0.85s: 216 chars, 6 segments\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Audio: 6 chunks\n",
      "  First chunk text: The stale smell of old beer lingers....\n",
      "  Has page_no: False\n",
      "  Has start_time: True\n",
      "\n",
      "✓ State Isolation Verified:\n",
      "  Document chunks != Audio chunks: True\n",
      "  loader.last_document is None: True\n",
      "  loader.last_transcription_result is not None: True\n"
     ]
    }
   ],
   "source": [
    "# Load and chunk document first\n",
    "print(\"Step 1: Processing document...\")\n",
    "doc_result = await multimodal_loader.run(pdf_path)\n",
    "doc_chunks = await multimodal_splitter.run(doc_result.text)\n",
    "\n",
    "print(f\"✓ Document: {len(doc_chunks.chunks)} chunks\")\n",
    "print(f\"  First chunk text: {doc_chunks.chunks[0].text[:80]}...\")\n",
    "print(f\"  Has page_no: {'page_no' in doc_chunks.chunks[0].metadata}\")\n",
    "print(f\"  Has start_time: {'start_time' in doc_chunks.chunks[0].metadata}\")\n",
    "\n",
    "# Now load and chunk audio (state should be isolated)\n",
    "print(\"\\nStep 2: Processing audio (after document)...\")\n",
    "audio_result = await multimodal_loader.run(audio_path)\n",
    "audio_chunks = await multimodal_splitter.run(audio_result.text)\n",
    "\n",
    "print(f\"✓ Audio: {len(audio_chunks.chunks)} chunks\")\n",
    "print(f\"  First chunk text: {audio_chunks.chunks[0].text[:80]}...\")\n",
    "print(f\"  Has page_no: {'page_no' in audio_chunks.chunks[0].metadata}\")\n",
    "print(f\"  Has start_time: {'start_time' in audio_chunks.chunks[0].metadata}\")\n",
    "\n",
    "# Verify state isolation\n",
    "print(\"\\n✓ State Isolation Verified:\")\n",
    "print(f\"  Document chunks != Audio chunks: {doc_chunks.chunks[0].text != audio_chunks.chunks[0].text}\")\n",
    "print(f\"  loader.last_document is None: {multimodal_loader.last_document is None}\")\n",
    "print(f\"  loader.last_transcription_result is not None: {multimodal_loader.last_transcription_result is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Audio → Document (Reverse Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mHardware detected: mps\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mInitialized UniversalConverter on mps\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_ensure_models_available\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mChecking model availability...\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.audio_factory\u001b[0m:\u001b[36mcreate_audio_transcriber\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mCreating audio transcriber: model=base, backend=auto\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mInitializing WhisperTranscriber with model=base, device=mlx, backend=auto\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading tokenizer: Qwen/Qwen3-Embedding-0.6B (padding_side=left)\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mInitialized HybridChunker with max_tokens=512, merge_peers=True\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m_load_model\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mLoading Whisper model: base\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36m_load_model\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mLoaded MLX Whisper model: base\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36mtranscribe\u001b[0m:\u001b[36m139\u001b[0m - \u001b[1mTranscribing harvard.wav with mlx backend\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Processing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[32m2026-01-03 16:57:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.audio.whisper_transcriber\u001b[0m:\u001b[36mtranscribe\u001b[0m:\u001b[36m193\u001b[0m - \u001b[1mTranscription complete in 0.60s: 216 chars, 6 segments\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mUsing Classic pipeline (default) for 2112.13734v2.pdf\u001b[0m\n",
      "\u001b[32m2026-01-03 16:57:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting 2112.13734v2.pdf with classic pipeline\u001b[0m\n",
      "2026-01-03 16:57:57,994 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-03 16:57:57,996 - INFO - Going to convert document batch...\n",
      "2026-01-03 16:57:57,997 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-03 16:57:57,997 - INFO - Accelerator device: 'mps'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Audio: 6 chunks\n",
      "  Modality: audio\n",
      "\n",
      "Step 2: Processing document (after audio)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 16:57:58,490 - INFO - Accelerator device: 'mps'\n",
      "2026-01-03 16:57:59,148 - INFO - Processing document 2112.13734v2.pdf\n",
      "2026-01-03 16:58:04,327 - INFO - Finished converting document 2112.13734v2.pdf in 6.33 sec.\n",
      "\u001b[32m2026-01-03 16:58:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted 2112.13734v2.pdf in 6.39s (4 pages, 0.6 pages/sec)\u001b[0m\n",
      "\u001b[32m2026-01-03 16:58:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mChunking document: 2112.13734v2.pdf (4 pages)\u001b[0m\n",
      "\u001b[32m2026-01-03 16:58:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mCreated 22 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Document: 22 chunks\n",
      "  Has page_no: True\n",
      "\n",
      "✓ Reverse State Isolation Verified:\n",
      "  Audio chunks != Document chunks: True\n",
      "  loader.last_document is not None: True\n",
      "  loader.last_transcription_result is None: True\n"
     ]
    }
   ],
   "source": [
    "# Create fresh loader\n",
    "fresh_loader = MultimodalLoader()\n",
    "fresh_splitter = VertectorTextSplitter(loader=fresh_loader, chunk_size=512)\n",
    "\n",
    "# Load audio first\n",
    "print(\"Step 1: Processing audio...\")\n",
    "audio_result = await fresh_loader.run(audio_path)\n",
    "audio_chunks = await fresh_splitter.run(audio_result.text)\n",
    "\n",
    "print(f\"✓ Audio: {len(audio_chunks.chunks)} chunks\")\n",
    "print(f\"  Modality: {audio_chunks.chunks[0].metadata.get('modality', 'N/A')}\")\n",
    "\n",
    "# Then load document\n",
    "print(\"\\nStep 2: Processing document (after audio)...\")\n",
    "doc_result = await fresh_loader.run(pdf_path)\n",
    "doc_chunks = await fresh_splitter.run(doc_result.text)\n",
    "\n",
    "print(f\"✓ Document: {len(doc_chunks.chunks)} chunks\")\n",
    "print(f\"  Has page_no: {'page_no' in doc_chunks.chunks[0].metadata}\")\n",
    "\n",
    "# Verify state isolation\n",
    "print(\"\\n✓ Reverse State Isolation Verified:\")\n",
    "print(f\"  Audio chunks != Document chunks: {audio_chunks.chunks[0].text != doc_chunks.chunks[0].text}\")\n",
    "print(f\"  loader.last_document is not None: {fresh_loader.last_document is not None}\")\n",
    "print(f\"  loader.last_transcription_result is None: {fresh_loader.last_transcription_result is None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Metadata Comparison\n",
    "\n",
    "Compare the metadata between document and audio chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata Feature Comparison:\n",
      "        Feature Document Chunks Audio Chunks\n",
      "       chunk_id               ✓            ✓\n",
      "    document_id               ✓            ✓\n",
      "    token_count               ✓            ✓\n",
      "        page_no               ✓            ✗\n",
      "  section_title               ✗            ✗\n",
      "subsection_path               ✓            ✗\n",
      "       is_table               ✗            ✗\n",
      "     is_heading               ✗            ✗\n",
      "           bbox               ✓            ✗\n",
      "       modality               ✗            ✓\n",
      "     start_time               ✗            ✓\n",
      "       end_time               ✗            ✓\n",
      "       duration               ✗            ✓\n",
      "       language               ✗            ✓\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': [\n",
    "        'chunk_id',\n",
    "        'document_id', \n",
    "        'token_count',\n",
    "        'page_no',\n",
    "        'section_title',\n",
    "        'subsection_path',\n",
    "        'is_table',\n",
    "        'is_heading',\n",
    "        'bbox',\n",
    "        'modality',\n",
    "        'start_time',\n",
    "        'end_time',\n",
    "        'duration',\n",
    "        'language'\n",
    "    ],\n",
    "    'Document Chunks': [\n",
    "        '✓' if 'chunk_id' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'document_id' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'token_count' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'page_no' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'section_title' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'subsection_path' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_table' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_heading' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'bbox' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'modality' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'start_time' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'end_time' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'duration' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'language' in doc_chunks.chunks[0].metadata else '✗',\n",
    "    ],\n",
    "    'Audio Chunks': [\n",
    "        '✓' if 'chunk_id' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'document_id' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'token_count' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'page_no' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'section_title' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'subsection_path' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_table' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_heading' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'bbox' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'modality' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'start_time' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'end_time' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'duration' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'language' in audio_chunks.chunks[0].metadata else '✗',\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Metadata Feature Comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Neo4j Integration Example\n",
    "\n",
    "Demonstrates how to use these components with Neo4j SimpleKGPipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j SimpleKGPipeline Integration Pattern:\n",
      "\n",
      "\n",
      "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
      "from vertector_data_ingestion.integrations.neo4j import (\n",
      "    MultimodalLoader,\n",
      "    VertectorTextSplitter\n",
      ")\n",
      "\n",
      "# Initialize components\n",
      "loader = MultimodalLoader()\n",
      "splitter = VertectorTextSplitter(loader=loader, chunk_size=512)\n",
      "\n",
      "# Create Neo4j pipeline\n",
      "pipeline = SimpleKGPipeline(\n",
      "    llm=your_llm,\n",
      "    driver=your_neo4j_driver,\n",
      "    embedder=your_embedder,\n",
      "    entities=[...],\n",
      "    relations=[...],\n",
      "    from_pdf=False  # We handle loading ourselves\n",
      ")\n",
      "\n",
      "# Process document\n",
      "doc_result = await loader.run(Path(\"document.pdf\"))\n",
      "doc_chunks = await splitter.run(doc_result.text)\n",
      "\n",
      "# Process audio  \n",
      "audio_result = await loader.run(Path(\"meeting.wav\"))\n",
      "audio_chunks = await splitter.run(audio_result.text)\n",
      "\n",
      "# Feed to Neo4j pipeline\n",
      "await pipeline.run_async(\n",
      "    file_path=\"document.pdf\",\n",
      "    chunks=doc_chunks.chunks\n",
      ")\n",
      "\n",
      "await pipeline.run_async(\n",
      "    file_path=\"meeting.wav\",\n",
      "    chunks=audio_chunks.chunks\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: How to use with Neo4j SimpleKGPipeline\n",
    "print(\"Neo4j SimpleKGPipeline Integration Pattern:\\n\")\n",
    "\n",
    "example_code = '''\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "from vertector_data_ingestion.integrations.neo4j import (\n",
    "    MultimodalLoader,\n",
    "    VertectorTextSplitter\n",
    ")\n",
    "\n",
    "# Initialize components\n",
    "loader = MultimodalLoader()\n",
    "splitter = VertectorTextSplitter(loader=loader, chunk_size=512)\n",
    "\n",
    "# Create Neo4j pipeline\n",
    "pipeline = SimpleKGPipeline(\n",
    "    llm=your_llm,\n",
    "    driver=your_neo4j_driver,\n",
    "    embedder=your_embedder,\n",
    "    entities=[...],\n",
    "    relations=[...],\n",
    "    from_pdf=False  # We handle loading ourselves\n",
    ")\n",
    "\n",
    "# Process document\n",
    "doc_result = await loader.run(Path(\"document.pdf\"))\n",
    "doc_chunks = await splitter.run(doc_result.text)\n",
    "\n",
    "# Process audio  \n",
    "audio_result = await loader.run(Path(\"meeting.wav\"))\n",
    "audio_chunks = await splitter.run(audio_result.text)\n",
    "\n",
    "# Feed to Neo4j pipeline\n",
    "await pipeline.run_async(\n",
    "    file_path=\"document.pdf\",\n",
    "    chunks=doc_chunks.chunks\n",
    ")\n",
    "\n",
    "await pipeline.run_async(\n",
    "    file_path=\"meeting.wav\",\n",
    "    chunks=audio_chunks.chunks\n",
    ")\n",
    "'''\n",
    "\n",
    "print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✅ **Document Processing**\n",
    "- Structure-aware chunking with Docling HybridChunker\n",
    "- Rich metadata: page numbers, sections, bounding boxes, table detection\n",
    "\n",
    "✅ **Audio Processing**  \n",
    "- Whisper transcription with MLX acceleration\n",
    "- Segment-based chunking with timestamps\n",
    "- Audio metadata: start_time, end_time, duration, language\n",
    "\n",
    "✅ **Multimodal Pipeline**\n",
    "- Single loader handles both documents and audio\n",
    "- Automatic modality detection by file extension\n",
    "- Proper state isolation between modalities\n",
    "\n",
    "✅ **Neo4j Integration**\n",
    "- Compatible with Neo4j SimpleKGPipeline\n",
    "- Preserves rich metadata for knowledge graph construction\n",
    "- Ready for production use\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Property Delegation**: MultimodalLoader properly exposes sub-loader state\n",
    "2. **State Isolation**: Loading one modality clears the state of the other\n",
    "3. **Metadata Preservation**: All rich metadata flows through to Neo4j chunks\n",
    "4. **Document IDs**: Proper document_id, chunk_id for both documents and audio\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Connect to Neo4j database\n",
    "- Define entity and relation schemas\n",
    "- Build knowledge graph from multimodal data\n",
    "- Query and visualize the graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
