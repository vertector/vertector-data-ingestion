{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j SimpleKGPipeline Integration\n",
    "\n",
    "This notebook demonstrates the complete Neo4j integration with Vertector, showcasing:\n",
    "- Document loading and chunking with rich metadata\n",
    "- Audio transcription and segment-based chunking\n",
    "- Multimodal pipeline handling both documents and audio\n",
    "- State isolation between different modalities\n",
    "- Metadata preservation through the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from vertector_data_ingestion import setup_logging\n",
    "from vertector_data_ingestion.integrations.neo4j import (\n",
    "    MultimodalLoader,\n",
    "    VertectorAudioLoader,\n",
    "    VertectorDataLoader,\n",
    "    VertectorTextSplitter,\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(log_level=\"INFO\")\n",
    "\n",
    "print(\"✓ Neo4j integration components imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Document Processing\n",
    "\n",
    "Load and chunk a PDF document using Docling's structure-aware chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize document loader and splitter\n",
    "doc_loader = VertectorDataLoader()\n",
    "doc_splitter = VertectorTextSplitter(loader=doc_loader, chunk_size=512)\n",
    "\n",
    "# Load document\n",
    "pdf_path = Path(\"../test_documents/2112.13734v2.pdf\")\n",
    "print(f\"Loading document: {pdf_path.name}\")\n",
    "\n",
    "doc_result = await doc_loader.run(pdf_path)\n",
    "\n",
    "print(f\"\\n✓ Document loaded:\")\n",
    "print(f\"  Type: {doc_result.document_info.document_type}\")\n",
    "print(f\"  Pages: {doc_result.document_info.metadata['num_pages']}\")\n",
    "print(f\"  Processing time: {doc_result.document_info.metadata['processing_time']}s\")\n",
    "print(f\"  Text length: {len(doc_result.text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the document\n",
    "print(\"Chunking document with structure-aware HybridChunker...\")\n",
    "doc_chunks = await doc_splitter.run(doc_result.text)\n",
    "\n",
    "print(f\"\\n✓ Created {len(doc_chunks.chunks)} document chunks\")\n",
    "print(f\"\\nFirst chunk:\")\n",
    "print(f\"  Text: {doc_chunks.chunks[0].text[:100]}...\")\n",
    "print(f\"  Index: {doc_chunks.chunks[0].index}\")\n",
    "print(f\"  Metadata keys: {list(doc_chunks.chunks[0].metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine rich metadata from multiple chunks\n",
    "print(\"Document chunk metadata examples:\\n\")\n",
    "\n",
    "for i in range(min(3, len(doc_chunks.chunks))):\n",
    "    chunk = doc_chunks.chunks[i]\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Token count: {chunk.metadata.get('token_count')}\")\n",
    "    print(f\"  Page number: {chunk.metadata.get('page_no', 'N/A')}\")\n",
    "    print(f\"  Section: {chunk.metadata.get('subsection_path', 'N/A')[:80]}\")\n",
    "    print(f\"  Is table: {chunk.metadata.get('is_table', 'False')}\")\n",
    "    print(f\"  Is heading: {chunk.metadata.get('is_heading', 'False')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Audio Processing\n",
    "\n",
    "Transcribe and chunk audio using Whisper segments with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize audio loader and splitter\n",
    "audio_loader = VertectorAudioLoader()\n",
    "audio_splitter = VertectorTextSplitter(loader=audio_loader, chunk_size=512)\n",
    "\n",
    "# Load audio\n",
    "audio_path = Path(\"../test_documents/harvard.wav\")\n",
    "print(f\"Loading audio: {audio_path.name}\")\n",
    "\n",
    "audio_result = await audio_loader.run(audio_path)\n",
    "\n",
    "print(f\"\\n✓ Audio loaded:\")\n",
    "print(f\"  Type: {audio_result.document_info.document_type}\")\n",
    "print(f\"  Duration: {audio_result.document_info.metadata['duration']}s\")\n",
    "print(f\"  Language: {audio_result.document_info.metadata['language']}\")\n",
    "print(f\"  Segments: {audio_result.document_info.metadata['segments']}\")\n",
    "print(f\"  Model: {audio_result.document_info.metadata['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the audio\n",
    "print(\"Chunking audio using Whisper segments...\")\n",
    "audio_chunks = await audio_splitter.run(audio_result.text)\n",
    "\n",
    "print(f\"\\n✓ Created {len(audio_chunks.chunks)} audio chunks\")\n",
    "print(f\"\\nFirst chunk:\")\n",
    "print(f\"  Text: {audio_chunks.chunks[0].text}\")\n",
    "print(f\"  Index: {audio_chunks.chunks[0].index}\")\n",
    "print(f\"  Metadata keys: {list(audio_chunks.chunks[0].metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine audio chunk metadata with timestamps\n",
    "print(\"Audio chunk metadata (with timestamps):\\n\")\n",
    "\n",
    "for i, chunk in enumerate(audio_chunks.chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Text: {chunk.text}\")\n",
    "    print(f\"  Start: {chunk.metadata['start_time']}s\")\n",
    "    print(f\"  End: {chunk.metadata['end_time']}s\")\n",
    "    print(f\"  Duration: {chunk.metadata['duration']}s\")\n",
    "    print(f\"  Token count: {chunk.metadata['token_count']}\")\n",
    "    print(f\"  Document ID: {chunk.metadata['document_id']}\")\n",
    "    print(f\"  Chunk ID: {chunk.metadata['chunk_id']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multimodal Pipeline\n",
    "\n",
    "Use a single MultimodalLoader to handle both documents and audio, with automatic modality detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multimodal loader and splitter\n",
    "multimodal_loader = MultimodalLoader()\n",
    "multimodal_splitter = VertectorTextSplitter(loader=multimodal_loader, chunk_size=512)\n",
    "\n",
    "print(\"✓ Multimodal pipeline initialized\")\n",
    "print(\"  Supports: PDF, DOCX, PPTX, XLSX, WAV, MP3, M4A, FLAC, OGG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Document → Audio (State Isolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk document first\n",
    "print(\"Step 1: Processing document...\")\n",
    "doc_result = await multimodal_loader.run(pdf_path)\n",
    "doc_chunks = await multimodal_splitter.run(doc_result.text)\n",
    "\n",
    "print(f\"✓ Document: {len(doc_chunks.chunks)} chunks\")\n",
    "print(f\"  First chunk text: {doc_chunks.chunks[0].text[:80]}...\")\n",
    "print(f\"  Has page_no: {'page_no' in doc_chunks.chunks[0].metadata}\")\n",
    "print(f\"  Has start_time: {'start_time' in doc_chunks.chunks[0].metadata}\")\n",
    "\n",
    "# Now load and chunk audio (state should be isolated)\n",
    "print(\"\\nStep 2: Processing audio (after document)...\")\n",
    "audio_result = await multimodal_loader.run(audio_path)\n",
    "audio_chunks = await multimodal_splitter.run(audio_result.text)\n",
    "\n",
    "print(f\"✓ Audio: {len(audio_chunks.chunks)} chunks\")\n",
    "print(f\"  First chunk text: {audio_chunks.chunks[0].text[:80]}...\")\n",
    "print(f\"  Has page_no: {'page_no' in audio_chunks.chunks[0].metadata}\")\n",
    "print(f\"  Has start_time: {'start_time' in audio_chunks.chunks[0].metadata}\")\n",
    "\n",
    "# Verify state isolation\n",
    "print(\"\\n✓ State Isolation Verified:\")\n",
    "print(f\"  Document chunks != Audio chunks: {doc_chunks.chunks[0].text != audio_chunks.chunks[0].text}\")\n",
    "print(f\"  loader.last_document is None: {multimodal_loader.last_document is None}\")\n",
    "print(f\"  loader.last_transcription_result is not None: {multimodal_loader.last_transcription_result is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Audio → Document (Reverse Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh loader\n",
    "fresh_loader = MultimodalLoader()\n",
    "fresh_splitter = VertectorTextSplitter(loader=fresh_loader, chunk_size=512)\n",
    "\n",
    "# Load audio first\n",
    "print(\"Step 1: Processing audio...\")\n",
    "audio_result = await fresh_loader.run(audio_path)\n",
    "audio_chunks = await fresh_splitter.run(audio_result.text)\n",
    "\n",
    "print(f\"✓ Audio: {len(audio_chunks.chunks)} chunks\")\n",
    "print(f\"  Modality: {audio_chunks.chunks[0].metadata.get('modality', 'N/A')}\")\n",
    "\n",
    "# Then load document\n",
    "print(\"\\nStep 2: Processing document (after audio)...\")\n",
    "doc_result = await fresh_loader.run(pdf_path)\n",
    "doc_chunks = await fresh_splitter.run(doc_result.text)\n",
    "\n",
    "print(f\"✓ Document: {len(doc_chunks.chunks)} chunks\")\n",
    "print(f\"  Has page_no: {'page_no' in doc_chunks.chunks[0].metadata}\")\n",
    "\n",
    "# Verify state isolation\n",
    "print(\"\\n✓ Reverse State Isolation Verified:\")\n",
    "print(f\"  Audio chunks != Document chunks: {audio_chunks.chunks[0].text != doc_chunks.chunks[0].text}\")\n",
    "print(f\"  loader.last_document is not None: {fresh_loader.last_document is not None}\")\n",
    "print(f\"  loader.last_transcription_result is None: {fresh_loader.last_transcription_result is None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Metadata Comparison\n",
    "\n",
    "Compare the metadata between document and audio chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': [\n",
    "        'chunk_id',\n",
    "        'document_id', \n",
    "        'token_count',\n",
    "        'page_no',\n",
    "        'section_title',\n",
    "        'subsection_path',\n",
    "        'is_table',\n",
    "        'is_heading',\n",
    "        'bbox',\n",
    "        'modality',\n",
    "        'start_time',\n",
    "        'end_time',\n",
    "        'duration',\n",
    "        'language'\n",
    "    ],\n",
    "    'Document Chunks': [\n",
    "        '✓' if 'chunk_id' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'document_id' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'token_count' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'page_no' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'section_title' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'subsection_path' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_table' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_heading' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'bbox' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'modality' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'start_time' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'end_time' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'duration' in doc_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'language' in doc_chunks.chunks[0].metadata else '✗',\n",
    "    ],\n",
    "    'Audio Chunks': [\n",
    "        '✓' if 'chunk_id' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'document_id' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'token_count' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'page_no' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'section_title' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'subsection_path' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_table' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'is_heading' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'bbox' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'modality' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'start_time' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'end_time' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'duration' in audio_chunks.chunks[0].metadata else '✗',\n",
    "        '✓' if 'language' in audio_chunks.chunks[0].metadata else '✗',\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Metadata Feature Comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Neo4j Integration Example\n",
    "\n",
    "Demonstrates how to use these components with Neo4j SimpleKGPipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to use with Neo4j SimpleKGPipeline\n",
    "print(\"Neo4j SimpleKGPipeline Integration Pattern:\\n\")\n",
    "\n",
    "example_code = '''\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "from vertector_data_ingestion.integrations.neo4j import (\n",
    "    MultimodalLoader,\n",
    "    VertectorTextSplitter\n",
    ")\n",
    "\n",
    "# Initialize components\n",
    "loader = MultimodalLoader()\n",
    "splitter = VertectorTextSplitter(loader=loader, chunk_size=512)\n",
    "\n",
    "# Create Neo4j pipeline\n",
    "pipeline = SimpleKGPipeline(\n",
    "    llm=your_llm,\n",
    "    driver=your_neo4j_driver,\n",
    "    embedder=your_embedder,\n",
    "    entities=[...],\n",
    "    relations=[...],\n",
    "    from_pdf=False  # We handle loading ourselves\n",
    ")\n",
    "\n",
    "# Process document\n",
    "doc_result = await loader.run(Path(\"document.pdf\"))\n",
    "doc_chunks = await splitter.run(doc_result.text)\n",
    "\n",
    "# Process audio  \n",
    "audio_result = await loader.run(Path(\"meeting.wav\"))\n",
    "audio_chunks = await splitter.run(audio_result.text)\n",
    "\n",
    "# Feed to Neo4j pipeline\n",
    "await pipeline.run_async(\n",
    "    file_path=\"document.pdf\",\n",
    "    chunks=doc_chunks.chunks\n",
    ")\n",
    "\n",
    "await pipeline.run_async(\n",
    "    file_path=\"meeting.wav\",\n",
    "    chunks=audio_chunks.chunks\n",
    ")\n",
    "'''\n",
    "\n",
    "print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✅ **Document Processing**\n",
    "- Structure-aware chunking with Docling HybridChunker\n",
    "- Rich metadata: page numbers, sections, bounding boxes, table detection\n",
    "\n",
    "✅ **Audio Processing**  \n",
    "- Whisper transcription with MLX acceleration\n",
    "- Segment-based chunking with timestamps\n",
    "- Audio metadata: start_time, end_time, duration, language\n",
    "\n",
    "✅ **Multimodal Pipeline**\n",
    "- Single loader handles both documents and audio\n",
    "- Automatic modality detection by file extension\n",
    "- Proper state isolation between modalities\n",
    "\n",
    "✅ **Neo4j Integration**\n",
    "- Compatible with Neo4j SimpleKGPipeline\n",
    "- Preserves rich metadata for knowledge graph construction\n",
    "- Ready for production use\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Property Delegation**: MultimodalLoader properly exposes sub-loader state\n",
    "2. **State Isolation**: Loading one modality clears the state of the other\n",
    "3. **Metadata Preservation**: All rich metadata flows through to Neo4j chunks\n",
    "4. **Document IDs**: Proper document_id, chunk_id for both documents and audio\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Connect to Neo4j database\n",
    "- Define entity and relation schemas\n",
    "- Build knowledge graph from multimodal data\n",
    "- Query and visualize the graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
