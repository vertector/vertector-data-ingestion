{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Vertector\n",
    "\n",
    "Complete RAG pipeline including:\n",
    "- Document chunking\n",
    "- Vector store integration\n",
    "- Semantic search\n",
    "- Batch ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 02:06:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.monitoring.logger\u001b[0m:\u001b[36msetup_logging\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mLogging initialized at INFO level\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from vertector_data_ingestion import (\n",
    "    UniversalConverter,\n",
    "    LocalMpsConfig,\n",
    "    HybridChunker,\n",
    "    ChromaAdapter,\n",
    "    ExportFormat,\n",
    "    setup_logging,\n",
    ")\n",
    "from vertector_data_ingestion.models.config import ChunkingConfig\n",
    "\n",
    "setup_logging(log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 02:06:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-03 02:06:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.hardware_detector\u001b[0m:\u001b[36mdetect\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mDetected Apple Silicon with MPS support\u001b[0m\n",
      "\u001b[32m2026-01-03 02:06:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mHardware detected: mps\u001b[0m\n",
      "\u001b[32m2026-01-03 02:06:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mInitialized UniversalConverter on mps\u001b[0m\n",
      "\u001b[32m2026-01-03 02:06:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_ensure_models_available\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mChecking model availability...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Converting Document\n",
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 02:06:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.pipeline_router\u001b[0m:\u001b[36mdetermine_pipeline\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mUsing Classic pipeline for PDF with tables: arxiv_sample.pdf\u001b[0m\n",
      "\u001b[32m2026-01-03 02:06:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mConverting arxiv_sample.pdf with classic pipeline\u001b[0m\n",
      "2026-01-03 02:06:38,377 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-03 02:06:38,459 - INFO - Going to convert document batch...\n",
      "2026-01-03 02:06:38,460 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 870e160bad93d15722a8ae8d62725e09\n",
      "2026-01-03 02:06:38,480 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-03 02:06:38,482 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-03 02:06:38,490 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-03 02:06:38,502 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-03 02:06:45,199 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-03 02:06:45,205 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-03 02:06:45,217 - INFO - Accelerator device: 'mps'\n",
      "2026-01-03 02:06:46,079 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-03 02:06:46,083 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-03 02:06:58,135 - INFO - Accelerator device: 'mps'\n",
      "2026-01-03 02:06:58,894 - INFO - Processing document arxiv_sample.pdf\n",
      "2026-01-03 02:07:11,441 - INFO - Finished converting document arxiv_sample.pdf in 33.07 sec.\n",
      "\u001b[32m2026-01-03 02:07:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.core.universal_converter\u001b[0m:\u001b[36m_convert_with_retry\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mConverted arxiv_sample.pdf in 37.43s (9 pages, 0.2 pages/sec)\u001b[0m\n",
      "\u001b[32m2026-01-03 02:07:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading tokenizer: Qwen/Qwen3-Embedding-0.6B (padding_side=left)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted: 9 pages\n",
      "\n",
      "Step 2: Creating Chunks\n",
      "Using tokenizer: Qwen/Qwen3-Embedding-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 02:07:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mInitialized HybridChunker with max_tokens=1024, merge_peers=True\u001b[0m\n",
      "\u001b[32m2026-01-03 02:07:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mChunking document: arxiv_sample.pdf (9 pages)\u001b[0m\n",
      "\u001b[32m2026-01-03 02:07:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.chunkers.hybrid_chunker\u001b[0m:\u001b[36mchunk_document\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mCreated 29 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: 29 chunks\n",
      "\n",
      "Step 3: Storing in Vector DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 02:07:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mLoading embedding model: Qwen/Qwen3-Embedding-0.6B\u001b[0m\n",
      "2026-01-03 02:07:19,303 - INFO - Use pytorch device_name: mps\n",
      "2026-01-03 02:07:19,304 - INFO - Load pretrained SentenceTransformer: Qwen/Qwen3-Embedding-0.6B\n",
      "2026-01-03 02:07:25,125 - INFO - 1 prompt is loaded, with the key: query\n",
      "2026-01-03 02:07:25,178 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "\u001b[32m2026-01-03 02:07:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mChromaDB initialized in-memory\u001b[0m\n",
      "\u001b[32m2026-01-03 02:07:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mUsing collection: rag_pipeline\u001b[0m\n",
      "\u001b[32m2026-01-03 02:07:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36madd_chunks\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mAdding 29 chunks to ChromaDB (batch_size=4)\u001b[0m\n",
      "\u001b[32m2026-01-03 02:08:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvertector_data_ingestion.vector.chroma_adapter\u001b[0m:\u001b[36madd_chunks\u001b[0m:\u001b[36m147\u001b[0m - \u001b[1mSuccessfully added 29 chunks in 8 batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Stored: 29 chunks\n",
      "\n",
      "Step 4: Semantic Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e3160f6d654b9297d338f8c8cfc214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "  Score: -0.243\n",
      "  Text: PDF document conversion, layout segmentation, object-detection, data set, Machine Learning...\n",
      "\n",
      "Result 2:\n",
      "  Score: -0.268\n",
      "  Text: Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: ...\n",
      "\n",
      "Result 3:\n",
      "  Score: -0.325\n",
      "  Text: Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
      "Christoph Auer IBM Researc...\n"
     ]
    }
   ],
   "source": [
    "# Configure chunker with Qwen3-Embedding-0.6B (smaller, faster)\n",
    "chunk_config = ChunkingConfig(\n",
    "    tokenizer=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "converter = UniversalConverter(LocalMpsConfig())\n",
    "doc_path = Path(\"../test_documents/arxiv_sample.pdf\")\n",
    "\n",
    "if doc_path.exists():\n",
    "    # Step 1: Convert\n",
    "    print(\"Step 1: Converting Document\")\n",
    "    doc = converter.convert(doc_path)\n",
    "    print(f\"✓ Converted: {doc.metadata.num_pages} pages\")\n",
    "    \n",
    "    # Step 2: Chunk with custom config\n",
    "    print(\"\\nStep 2: Creating Chunks\")\n",
    "    print(f\"Using tokenizer: {chunk_config.tokenizer}\")\n",
    "    chunker = HybridChunker(config=chunk_config)\n",
    "    chunks = chunker.chunk_document(doc)\n",
    "    print(f\"✓ Created: {chunks.total_chunks} chunks\")\n",
    "    \n",
    "    # Step 3: Store with matching embedding model\n",
    "    print(\"\\nStep 3: Storing in Vector DB\")\n",
    "    vector_store = ChromaAdapter(\n",
    "        collection_name=\"rag_pipeline\",\n",
    "        embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n",
    "    )\n",
    "    vector_store.add_chunks(chunks.chunks, batch_size=4)\n",
    "    print(f\"✓ Stored: {len(chunks.chunks)} chunks\")\n",
    "    \n",
    "    # Step 4: Search\n",
    "    print(\"\\nStep 4: Semantic Search\")\n",
    "    results = vector_store.search(\"Who is the main author of this paper?\", top_k=3)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"  Score: {result['score']:.3f}\")\n",
    "        print(f\"  Text: {result['text'][:100]}...\")\n",
    "else:\n",
    "    print(f\"File not found: {doc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks.chunks[0].chunk_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dir = Path(\"../test_documents/\")\n",
    "\n",
    "if documents_dir.exists():\n",
    "    pdf_files = list(documents_dir.glob(\"*.pdf\"))[:5]\n",
    "    \n",
    "    if pdf_files:\n",
    "        print(f\"Ingesting {len(pdf_files)} documents...\\n\")\n",
    "        \n",
    "        # Configure with Qwen3-Embedding-0.6B (default)\n",
    "        chunk_config = ChunkingConfig(\n",
    "            tokenizer=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        \n",
    "        converter = UniversalConverter()\n",
    "        chunker = HybridChunker(config=chunk_config)\n",
    "        vector_store = ChromaAdapter(\n",
    "            collection_name=\"rag_pipeline\",\n",
    "            embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n",
    "        )\n",
    "        \n",
    "        # Convert all documents\n",
    "        docs = converter.convert(pdf_files, parallel=True)\n",
    "        \n",
    "        # Chunk and store all\n",
    "        all_chunks = []\n",
    "        for doc in docs:\n",
    "            chunks = chunker.chunk_document(doc)\n",
    "            for chunk in chunks.chunks:\n",
    "                chunk.metadata[\"source_file\"] = doc.metadata.source_path.name\n",
    "            all_chunks.extend(chunks.chunks)\n",
    "        \n",
    "        vector_store.add_chunks(all_chunks, batch_size=4)\n",
    "        print(f\"\\n✓ Ingested {len(all_chunks)} chunks from {len(docs)} documents\")\n",
    "    else:\n",
    "        print(\"No PDF files found\")\n",
    "else:\n",
    "    print(\"Create a 'documents/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaAdapter(\n",
    "    collection_name=\"rag_pipeline\",\n",
    "    embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n",
    ")\n",
    "\n",
    "queries = [\n",
    "    \"Who are the authors of this paper?\",\n",
    "    \"What are the main findings?\",\n",
    "    \"What is the paper about?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = vector_store.search(query, top_k=1)\n",
    "    if results:\n",
    "        print(f\"Q: {query}\")\n",
    "        print(f\"A: {results[0]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Demonstrated:\n",
    "- Complete RAG pipeline\n",
    "- Batch document ingestion with unified `convert()`\n",
    "- Vector search\n",
    "\n",
    "Next: `04_multimodal_integration.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
