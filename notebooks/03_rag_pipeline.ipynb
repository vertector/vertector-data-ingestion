{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Vertector\n",
    "\n",
    "Complete RAG pipeline including:\n",
    "- Document chunking\n",
    "- Vector store integration\n",
    "- Semantic search\n",
    "- Batch ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from vertector_data_ingestion import (\n",
    "    UniversalConverter,\n",
    "    LocalMpsConfig,\n",
    "    HybridChunker,\n",
    "    ChromaAdapter,\n",
    "    ExportFormat,\n",
    "    setup_logging,\n",
    ")\n",
    "from vertector_data_ingestion.models.config import ChunkingConfig\n",
    "\n",
    "setup_logging(log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure chunker with Qwen3-Embedding-0.6B (smaller, faster)\nchunk_config = ChunkingConfig(\n    tokenizer=\"Qwen/Qwen3-Embedding-0.6B\",\n    max_tokens=512,\n)\n\nconverter = UniversalConverter(LocalMpsConfig())\ndoc_path = Path(\"../test_documents/arxiv_sample.pdf\")\n\nif doc_path.exists():\n    # Step 1: Convert\n    print(\"Step 1: Converting Document\")\n    doc = converter.convert(doc_path)\n    print(f\"✓ Converted: {doc.metadata.num_pages} pages\")\n    \n    # Step 2: Chunk with custom config\n    print(\"\\nStep 2: Creating Chunks\")\n    print(f\"Using tokenizer: {chunk_config.tokenizer}\")\n    chunker = HybridChunker(config=chunk_config)\n    chunks = chunker.chunk_document(doc)\n    print(f\"✓ Created: {chunks.total_chunks} chunks\")\n    \n    # Step 3: Store with matching embedding model\n    print(\"\\nStep 3: Storing in Vector DB\")\n    vector_store = ChromaAdapter(\n        collection_name=\"rag_pipeline\",\n        embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n    )\n    vector_store.add_chunks(chunks.chunks, batch_size=4)\n    print(f\"✓ Stored: {len(chunks.chunks)} chunks\")\n    \n    # Step 4: Search\n    print(\"\\nStep 4: Semantic Search\")\n    results = vector_store.search(\"Who is the main author of this paper?\", top_k=3)\n    for i, result in enumerate(results, 1):\n        print(f\"\\nResult {i}:\")\n        print(f\"  Score: {result['score']:.3f}\")\n        print(f\"  Text: {result['text'][:100]}...\")\nelse:\n    print(f\"File not found: {doc_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "documents_dir = Path(\"../test_documents/\")\n\nif documents_dir.exists():\n    pdf_files = list(documents_dir.glob(\"*.pdf\"))[:5]\n    \n    if pdf_files:\n        print(f\"Ingesting {len(pdf_files)} documents...\\n\")\n        \n        # Configure with Qwen3-Embedding-0.6B (default)\n        chunk_config = ChunkingConfig(\n            tokenizer=\"Qwen/Qwen3-Embedding-0.6B\",\n            max_tokens=512,\n        )\n        \n        converter = UniversalConverter()\n        chunker = HybridChunker(config=chunk_config)\n        vector_store = ChromaAdapter(\n            collection_name=\"rag_pipeline\",\n            embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n        )\n        \n        # Convert all documents\n        docs = converter.convert(pdf_files, parallel=True)\n        \n        # Chunk and store all\n        all_chunks = []\n        for doc in docs:\n            chunks = chunker.chunk_document(doc)\n            for chunk in chunks.chunks:\n                chunk.metadata[\"source_file\"] = doc.metadata.source_path.name\n            all_chunks.extend(chunks.chunks)\n        \n        vector_store.add_chunks(all_chunks, batch_size=4)\n        print(f\"\\n✓ Ingested {len(all_chunks)} chunks from {len(docs)} documents\")\n    else:\n        print(\"No PDF files found\")\nelse:\n    print(\"Create a 'documents/' directory\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "vector_store = ChromaAdapter(\n    collection_name=\"rag_pipeline\",\n    embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n)\n\nqueries = [\n    \"methodology\",\n    \"main findings\",\n    \"limitations\",\n]\n\nfor query in queries:\n    results = vector_store.search(query, top_k=1)\n    if results:\n        print(f\"Q: {query}\")\n        print(f\"A: {results[0]['text'][:150]}...\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Demonstrated:\n",
    "- Complete RAG pipeline\n",
    "- Batch document ingestion with unified `convert()`\n",
    "- Vector search\n",
    "\n",
    "Next: `04_multimodal_integration.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}