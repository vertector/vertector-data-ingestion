{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Vertector\n",
    "\n",
    "Complete RAG pipeline including:\n",
    "- Document chunking\n",
    "- Vector store integration\n",
    "- Semantic search\n",
    "- Batch ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from vertector_data_ingestion import (\n",
    "    UniversalConverter,\n",
    "    LocalMpsConfig,\n",
    "    HybridChunker,\n",
    "    ChromaAdapter,\n",
    "    ExportFormat,\n",
    "    setup_logging,\n",
    ")\n",
    "from vertector_data_ingestion.models.config import ChunkingConfig\n",
    "\n",
    "setup_logging(log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure chunker with Qwen3-Embedding-0.6B (smaller, faster)\n",
    "chunk_config = ChunkingConfig(\n",
    "    tokenizer=\"BAAI/bge-base-en-v1.5\",\n",
    "    max_tokens=768,\n",
    ")\n",
    "\n",
    "converter = UniversalConverter(LocalMpsConfig())\n",
    "doc_path = Path(\"../test_documents/arxiv_sample.pdf\")\n",
    "\n",
    "if doc_path.exists():\n",
    "    # Step 1: Convert\n",
    "    print(\"Step 1: Converting Document\")\n",
    "    doc = converter.convert(doc_path)\n",
    "    print(f\"✓ Converted: {doc.metadata.num_pages} pages\")\n",
    "    \n",
    "    # Step 2: Chunk with custom config\n",
    "    print(\"\\nStep 2: Creating Chunks\")\n",
    "    print(f\"Using tokenizer: {chunk_config.tokenizer}\")\n",
    "    chunker = HybridChunker(config=chunk_config)\n",
    "    chunks = chunker.chunk_document(doc)\n",
    "    print(f\"✓ Created: {chunks.total_chunks} chunks\")\n",
    "    \n",
    "    # Step 3: Store with matching embedding model\n",
    "    print(\"\\nStep 3: Storing in Vector DB\")\n",
    "    vector_store = ChromaAdapter(\n",
    "        collection_name=\"demo\",\n",
    "        embedding_model=\"BAAI/bge-base-en-v1.5\"\n",
    "    )\n",
    "    vector_store.add_chunks(chunks.chunks, batch_size=4)\n",
    "    print(f\"✓ Stored: {len(chunks.chunks)} chunks\")\n",
    "    \n",
    "    # Step 4: Search\n",
    "    print(\"\\nStep 4: Semantic Search\")\n",
    "    results = vector_store.search(\"Who is the main author of this paper?\", top_k=3)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"  Score: {result['score']:.3f}\")\n",
    "        print(f\"  Text: {result['text'][:100]}...\")\n",
    "else:\n",
    "    print(f\"File not found: {doc_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dir = Path(\"documents/\")\n",
    "\n",
    "if documents_dir.exists():\n",
    "    pdf_files = list(documents_dir.glob(\"*.pdf\"))[:5]\n",
    "    \n",
    "    if pdf_files:\n",
    "        print(f\"Ingesting {len(pdf_files)} documents...\\n\")\n",
    "        \n",
    "        # Configure with Qwen3-Embedding-0.6B\n",
    "        chunk_config = ChunkingConfig(\n",
    "            tokenizer=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        \n",
    "        converter = UniversalConverter()\n",
    "        chunker = HybridChunker(config=chunk_config)\n",
    "        vector_store = ChromaAdapter(\n",
    "            collection_name=\"batch_docs\",\n",
    "            embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n",
    "        )\n",
    "        \n",
    "        # Convert all documents\n",
    "        docs = converter.convert(pdf_files, parallel=True)\n",
    "        \n",
    "        # Chunk and store all\n",
    "        all_chunks = []\n",
    "        for doc in docs:\n",
    "            chunks = chunker.chunk_document(doc)\n",
    "            for chunk in chunks.chunks:\n",
    "                chunk.metadata[\"source_file\"] = doc.metadata.source_path.name\n",
    "            all_chunks.extend(chunks.chunks)\n",
    "        \n",
    "        vector_store.add_chunks(all_chunks)\n",
    "        print(f\"\\n✓ Ingested {len(all_chunks)} chunks from {len(docs)} documents\")\n",
    "    else:\n",
    "        print(\"No PDF files found\")\n",
    "else:\n",
    "    print(\"Create a 'documents/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaAdapter(\n",
    "    collection_name=\"demo\",\n",
    "    embedding_model=\"Qwen/Qwen3-Embedding-0.6B\"\n",
    ")\n",
    "\n",
    "queries = [\n",
    "    \"methodology\",\n",
    "    \"main findings\",\n",
    "    \"limitations\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = vector_store.search(query, top_k=1)\n",
    "    if results:\n",
    "        print(f\"Q: {query}\")\n",
    "        print(f\"A: {results[0]['text'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Demonstrated:\n",
    "- Complete RAG pipeline\n",
    "- Batch document ingestion with unified `convert()`\n",
    "- Vector search\n",
    "\n",
    "Next: `04_multimodal_integration.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
